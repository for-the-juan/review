{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lemonade Review Guidelines","text":""},{"location":"#about","title":"About","text":"<p>The New Lemonade Review is a paper review list maintained by prospective and current PhD students @pku-lemonade/phds at LEMONADE, Peking University. It is intended to provide a comprehensive overview of the quintessential research ideas in the field of computer architecture and systems.</p>"},{"location":"#how-to-find-the-papers","title":"How to find the papers?","text":"<p>Important</p> <p>Choose one of the first three strategies as your focus each week, depending on your current research needs.</p> <p>Important</p> <p>Check arXiv every day to make it a regular habit!</p>"},{"location":"#broad-search","title":"Broad Search","text":"<ul> <li>Goal: Get a broad overview of the topic.</li> <li>Method: Search in Google Scholar or read survey papers.</li> <li>Pace: 5+ papers/week (mostly skimming).</li> </ul>"},{"location":"#author-focused-search","title":"Author-Focused Search","text":"<ul> <li>Goal: Understand a key researcher's approach (methods, presentation, evaluation).</li> <li>Method: Search the author's papers in DBLP.</li> <li>Pace: 2-5 papers/week (skim first, then carefully read relevant papers).</li> </ul>"},{"location":"#citation-chasing","title":"Citation Chasing","text":"<ul> <li>Goal: Deep dive into a core paper (e.g., your intended main baseline).</li> <li>Method: Follow its main baselines/references (backward look) and check papers that cite it (forward look).</li> <li>Pace: 2-3 papers/week (requires careful reading).</li> <li>[!NOTE] Aim for at least 2 papers/week as you typically need to read the core paper alongside its main baseline.</li> </ul>"},{"location":"#arxiv-monitoring","title":"arXiv Monitoring","text":"<ul> <li>Goal: Stay updated with the latest research developments</li> <li>Method: Try AI immersive reading with Cool papers in the following categories:</li> <li>Distributed, Parallel, and Cluster Computing</li> <li>Emerging Technologies</li> <li>Hardware Architecture</li> <li>Performance</li> <li>Pace: 0+ papers/week (mostly skimming).</li> </ul>"},{"location":"#what-papers-should-be-included","title":"What papers should be included?","text":"<ul> <li>Peer-reviewed full papers ONLY.</li> <li>NO workshop papers, posters, or abstracts.</li> <li>Preprints are okay only for LLM-related topics.</li> </ul> <p>Important</p> <p>Ensure that at least half the papers are either published in CCF Rank A venues, or have a relavance score at least 3.</p>"},{"location":"#how-to-organize-the-papers","title":"How to organize the papers?","text":""},{"location":"#format","title":"Format","text":"<ul> <li>Sort papers by year in each subsection.</li> <li>CSV: Use quotes (\" \") around titles and tags containing commas \",\"</li> <li>PR Title/Description: Include search strategy, keywords, and note any papers read carefully (as opposed to skimmed).</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Use the affiliation of the corresponding author (or last author).</li> <li>Use standard, globally recognized abbreviations for affiliations.</li> </ul>"},{"location":"#subsections","title":"Subsections","text":"<ul> <li>Put papers only in Level 3 or Level 4 subsections.</li> <li>Group papers by a common challenge in each subsection. Briefly state that challenge at the beginning of the subsection.</li> <li>Min 2 papers per subsection. If only one fits, find a partner paper, or connect this paper to a related subsection.</li> <li>Max 5 papers per subsection. If more fit,  split the subsection.</li> </ul>"},{"location":"#tags","title":"Tags","text":"<ul> <li>Use specific tags for key techniques or contributions, such as \"xxx algorithm\", \"yyy model\", \"zzz architecture\", etc.</li> <li>Don't use broad area tags (\"performance\" or \"architecture\").</li> <li>Don't use vague feature tags (\"expresiveness\", \"scalability\", \"efficiency\")unless combined with a specific technique (e.g., \"xxx algorithm for scalability\").</li> <li>Explain potentially unclear acronyms (\"XYZ framework\") in tags.</li> </ul>"},{"location":"#links","title":"Links","text":"<ul> <li>Link related subsections (e.g., link hardware papers to related software papers sharing an idea, link compiler papers to related system papers sharing a technique).</li> </ul>"},{"location":"#how-to-read-the-papers-review-scores","title":"How to read the papers? (Review Scores)","text":""},{"location":"#presentation","title":"Presentation","text":"<p>Tip</p> <p>Skimming: Title, Abstract, Introduction, Figures.</p> <ul> <li>5: Definitely stealing some ideas for how they explained things or made their figures. I wish my papers looked this good.</li> <li>4: I will take note a figure or explanation and use in my next paper.</li> <li>3: I can write as good as it.</li> <li>2: Kind of a pain to read, or the figures are confusing/ugly. Hard to tell what's going on easily.</li> </ul>"},{"location":"#evaluation","title":"Evaluation","text":"<p>Tip</p> <p>Skimming: Experiments, Results, Reproducibility.</p> <ul> <li>5: Solid real-world tests. NVDA could actually use or bet on.</li> <li>4: Solid tests with real hardware. But the setup is not real-wrold ready.</li> <li>3: Okay tests with open source simulators. The results aren't obviously wrong.</li> <li>2: Okay tests with close source simulators. The results aren't obviously wrong.</li> <li>1: The results just don't make sense.</li> </ul>"},{"location":"#novelty","title":"Novelty","text":"<p>Note</p> <p>Requires careful reading: Assessing novelty means comparing the paper critically to related work. If you only skimmed the paper, either skip this score or assign at most 2.</p> <ul> <li>5: Totally new idea; groundbreaking.</li> <li>4: New take on existing ideas, or combines them smartly.</li> <li>3: An existing idea applied to a new area/topic.</li> <li>2: An existing idea applied to the same problem with standard incremental contributions.</li> <li>1: An existing idea applied to the same problem with minor variations.</li> </ul>"},{"location":"#relevance","title":"Relevance","text":"<p>Note</p> <p>Don't put the score in the review record because this score should be personal.</p> <ul> <li>5: Everyone need to read this paper</li> <li>4: I will put this paper on my side when I write my next paper</li> <li>3: I will cite this paper in my next paper</li> <li>2: I will cite this paper in my next survey paper</li> </ul>"},{"location":"hardware/","title":"Hardware","text":"<p>For each paper, identify its primary contribution and assign it to the single most appropriate category below.</p>"},{"location":"hardware/#processor-microarchitecture","title":"Processor Microarchitecture","text":"<p>Focuses on a single processing core and its components. Includes:</p> <ul> <li>Instruction set design</li> <li>Branch prediction</li> <li>Other core-level components and techniques</li> </ul>"},{"location":"hardware/#parallel-and-multi-processor-architecture","title":"Parallel and Multi-Processor Architecture","text":"<p>Covers systems with multiple processing units and their interactions. Includes:</p> <ul> <li>Multi-core processor architecture</li> <li>Many-core processor architecture</li> <li>GPU architecture</li> <li>Cache coherence protocols</li> <li>Memory consistency models</li> <li>System-level integration techniques such as:<ul> <li>Chiplets</li> <li>3D stacking</li> </ul> </li> </ul>"},{"location":"hardware/#memory-architecture","title":"Memory Architecture","text":"<p>Concerns memory subsystems and their interactions. Includes:</p> <ul> <li>Memory hierarchy design</li> <li>\u5b58\u7b97<ul> <li>Processing-in-Memory (PIM)</li> <li>Processing-Near-Memory (PNM)</li> <li>Computation-in-Memory (CIM)</li> </ul> </li> </ul>"},{"location":"hardware/#interconnection-networks","title":"Interconnection Networks","text":"<p>Addresses the communication fabric. Includes:</p> <ul> <li>Network-on-Chip (NoC) - topology, routing, flow control</li> <li>Note: Focus should be on the network itself, not primarily on using it to build a parallel architecture. Most CXL papers likely belong in other categories unless the primary contribution is the CXL protocol/architecture itself.</li> </ul>"},{"location":"hardware/#domain-specific-accelerators","title":"Domain-Specific Accelerators","text":"<p>Concerns accelerators tailored for specific applications. Includes:</p> <ul> <li>Reconfigurable architectures like FPGA/CGRA</li> <li>Accelerators for domains such as AI, graph processing, bioinformatics, etc.</li> </ul>"},{"location":"hardware/#security-and-reliability","title":"Security and Reliability","text":"<p>Covers hardware mechanisms for trust and resilience. Includes:</p> <ul> <li>Side-channel countermeasures (hardware aspects)</li> <li>Fault detection/mitigation hardware</li> </ul>"},{"location":"hardware/#emerging-technologies","title":"Emerging Technologies","text":"<p>Explores architectures based on novel technologies or paradigms. Includes:</p> <ul> <li>Quantum computing architectures</li> <li>Photonic computing architectures</li> <li>Note: CIM should be grouped under Memory Architecture.</li> </ul> <p>Error processing data file 'data/hardware/index.csv' referenced implicitly by page 'hardware/index.md'. Check build logs.</p>"},{"location":"hardware/accelerators/","title":"Domain-Specific Accelerators","text":""},{"location":"hardware/accelerators/#graph-accelerators","title":"Graph Accelerators","text":"<p>Challenge: Massive memory requirement, Non-ordered memory access</p> Year Venue Authors Title Tags P E N 2016 MICRO Princeton Graphicionado: A High-Performance and Energy-Efficient Accelerator for Graph Analytics vertex-programming based pipeline; on-chip scratchpad optimization; source/destination-oriented parallel streams 4 4 4"},{"location":"hardware/accelerators/#hypergraph-accelerators","title":"Hypergraph Accelerators","text":"<p>Solution: Realize the shared parts in hyperedges</p> Year Venue Authors Title Tags P E N 2022 MICRO HUST A Data-Centric Accelerator for High-Performance Hypergraph Processing Data-Centric; Load-Trigger-Reduce (LTR); Adaptive Data Loading 4 4 3 2025 HPCA HUST MeHyper: Accelerating Hypergraph Neural Networks by Exploring Implicit Dataflows Microedge; Microedge-Centric Dataflow; RePAG Execution Model 4 3 4"},{"location":"hardware/accelerators/#dnn-accelerators","title":"DNN Accelerators","text":""},{"location":"hardware/accelerators/#layer-fusion-accelerators","title":"Layer Fusion Accelerators","text":"<p>Solution: Use layer fusion to combine multiple layers of a neural network into a single layer. This can help reduce the number of computations and memory accesses required during inference; leading to faster execution times and lower power consumption.</p> Year Venue Authors Title Tags P E N 2016 MICRO SBU Fused-Layer CNN Accelerators fuse the processing of multiple CNN layers by modifying the order in which the input data are brought on chip 2025 TC KU Leuven Stream: Design Space Exploration of Layer-Fused DNNs on Heterogeneous Dataflow Accelerators fine-grain mapping paradigm; mapping of layer-fused DNNs on heterogeneous dataflow accelerator architectures; memory- and communication-aware latency analysis; constraint optimization 2024 SOCC IIT Hyderabad Hardware-Aware Network Adaptation using Width and Depth Shrinking including Convolutional and Fully Connected Layer Merging Width Shrinking: reduces the number of feature maps in CNN layers; Depth Shrinking: Merge of conv layer and fc layer 2024 ICSAI MIT LoopTree: Exploring the Fused-Layer Dataflow  Accelerator Design Space design space that supports set of tiling, recomputation, retention choices, and their combinations; model that validates design space"},{"location":"hardware/accelerators/#llm-accelerators","title":"LLM Accelerators","text":"<p>Challenge: LLM accelerators face challenges in terms of memory bandwidth; power consumption; and the need for efficient data movement.</p> Year Venue Authors Title Tags P E N 2024 DATE NTU ViTA: A Highly Efficient Dataflow and Architecture for Vision Transformers highly efficient memory-centric dataflow; fused special function module for non-linear functions; A comprehensive DSE of ViTA Kernels and VMUs 2025 arXiv SJTU ROMA: A Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM hybrid ROM-SRAM architecture for on-device LLM; B-ROM design for area-efficient ROM; fused cell integration of ROM and compute unit; QLoRA rank adaptation for task-specific tuning; on-chip storage optimization for quantized models"},{"location":"hardware/accelerators/#quantized-dnn-accelerators","title":"Quantized DNN Accelerators","text":"<p>Solution: Quantized DNN accelerators are designed to efficiently execute quantized neural networks, which use lower precision representations for weights and activations.</p> Year Venue Authors Title Tags P E N 2018 ISCA SNU Energy-Efficient Neural Network Accelerator Based on Outlier-Aware Low-Precision Computation accelerator architecture for outlier-aware quantized models; outlier-aware low-precision computation; separate outlier MAC unit 4 3 2 2018 ISCA Georgia Tech Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Network accelerator for layer-aware quantized DNN; bit-flexible computation unit; block-structured instruction set architecture 4 3 3 2024 DAC ASU Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference composite data type Logarithmic Posits (LP); automated post training LP Quantization (LPQ) Framework based on genetic algorithms; mixed-precision LP Accelerator (LPA) 3 3 2"},{"location":"hardware/accelerators/#benchmarks","title":"Benchmarks","text":"Year Venue Authors Title Tags P E N 2025 arXiv Cambridge Benchmarking Ultra-Low-Power \u00b5NPUs Comparative \u00b5NPU Benchmarking (\u00b5NPU: microcontroller-scale Neural Processing Unit); open-source model compilation framework; \u00b5NPU memory I/O bottleneck identification 4 4 2"},{"location":"hardware/accelerators/#dataflow-architecture","title":"Dataflow Architecture","text":"<p>Solution: Dataflow architecture allows the execution of instructions based on the availability of data rather than a predetermined sequence; leading to more efficient use of resources and better performance in parallel processing and real-time systems.</p> Year Venue Authors Title Tags P E N 2019 ASPLOS THU Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators buffer sharing dataflow(BSD); alternate layer loop ordering (ALLO) dataflow; heuristics spatial layer mapping algorithm 2024 MICRO CMU The TYR Dataflow Architecture: Improving Locality by Taming Parallelism local tag spaces technique; space tag managing instruction set; CT based concurrent-block communication 2024 MICRO UCR Sparsepipe: Sparse Inter-operator Dataflow Architecture with Cross-Iteration Reuse producer-consumer reuse; cross-iteration reuse; sub-tensor dependency; OEI dataflow; sparsepipe architecture 2025 arXiv UCSB FETTA: Flexible and Efficient Hardware Accelerator for Tensorized Neural Network Training contraction sequence search engine; tensor contraction unit; distribution/reduction network 3 4 3 2025 ISCA PKU H2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference operator-channel binding; computation-andwidth trade-off; dataflow-based DSE 4 3 3"},{"location":"hardware/accelerators/#data-mapping","title":"Data Mapping","text":"<p>Solution: Assign data to specific locations in memory or storage to optimize performance; reduce latency; and improve resource utilization.</p>"},{"location":"hardware/accelerators/#survey","title":"Survey","text":"Year Venue Authors Title Tags P E N 2013 DAC NUS Mapping on Multi/Many-core Systems: Survey of Current and Emerging Trends dense/run-time mapping; centralized/distributred management; hybrid mapping"},{"location":"hardware/accelerators/#heuristic-algorithm","title":"Heuristic Algorithm","text":"Year Venue Authors Title Tags P E N 2021 HPCA Georgia Tech MAGMA: An Optimization Framework for Mapping Multiple DNNs on Multiple Accelerator Cores sub-accelerator selection; fine-grained job prioritization; MANGA crossover genetic operators 2023 ISCA THU MapZero: Mapping for Coarse-grained Reconfigurable Architectures with Reinforcement Learning and Monte-Carlo Tree Search GAT based DFG and CGRA embedding; routing penalty based reinforcement learning; Monte-Carlo tree search space exploration 2023 VLSI IIT Kharagpur Application Mapping Onto Manycore Processor Architectures Using Active Search Framework RNN based active search framework; IP-Core Numbering Scheme; active search with/without pretraining"},{"location":"hardware/accelerators/#optimization-modeling","title":"Optimization Modeling","text":"Year Venue Authors Title Tags P E N 2020 FPGA ETH Zurich Flexible Communication Avoiding Matrix Multiplication on FPGA with High-Level Synthesis computation and I/O decomposition model for matrix multiplication; 1D array collapse mapping method; internal double buffering 2021 HPCA Georgia Tech Heterogeneous Dataflow Accelerators for Multi-DNN Workloads heterogeneous dataflow accelerators (HDAs) for DNN; dataflow flexibility; high utilization across the sub-accelerators 2023 MICRO Alibaba; CUHK ArchExplorer: Microarchitecture Exploration Via Bottleneck Analysis dynamic event-dependence graph(EDG); induced DEG based critical path construction; bottleneck-removal-driven DSE 2023 ISCA THU Inter-layer Scheduling Space Definition and Exploration for Tiled Accelerators inter-layer encoding method; temperal cut; spatial cut; RA tree analysis"},{"location":"hardware/accelerators/#fault-tolerant-mapping","title":"Fault Tolerant Mapping","text":"Year Venue Authors Title Tags P E N 2017 SC NIT High-performance and energy-efficient fault-tolerance core mapping in NoC weighted communication energy; placing unmapped vertices region; application core graph; spare core placement algorithm 2019 IVLSI UESTC Optimized mapping algorithm to extend lifetime of both NoC and cores in many-core system lifetime budget metric; LBC-LBL mapping algorithm; electro-migration fault model"},{"location":"hardware/accelerators/#reliability-management","title":"Reliability Management","text":"Year Venue Authors Title Tags P E N 2020 DATE Turku Thermal-Cycling-aware Dynamic Reliability Management in -Core System-on-Chip Coffin-Mason equation based reliability model; reliability-aware mapping/scheduling; dynamic power management 2024 arXiv WUSTL A Two-Level Thermal Cycling-Aware Task Mapping Technique for Reliability Management in Manycore Systems temperature based bin packing; task-to-bin assignment; thermal cycling-aware based task-to-core mapping 2024 arXiv WUSTL A Reinforcement Learning-Based Task Mapping Method to Improve the Reliability of Clustered Manycores mean time to failure; density-based spatial clustering of applications with noise algorithm"},{"location":"hardware/accelerators/#task-scheduling","title":"Task Scheduling","text":"Year Venue Authors Title Tags P E N 2023 ICCAD PKU Memory-aware Scheduling for Complex Wired Networks with Iterative Graph Optimization topology-aware pruning algorithm; integer linear programming scheduling method; sub-graph fusion algorithm ; memory-aware graph partitioning 2023 MICRO Duke Si-Kintsugi: Towards Recovering Golden-Like Performance of Defective Many-Core Spatial Architectures for AI graph alignment algorithm for dataflow graph and platform pe grap; producer-consumer pattern dataflow generation algorithm"},{"location":"hardware/accelerators/#many-core-architecture","title":"Many-core Architecture","text":"<p>Challenge: Many-core architectures are designed to handle a large number of cores; but they face challenges in terms of power consumption; performance; and resource allocation.</p> Year Venue Authors Title Tags P E N 2015 HPCA Cornel Increasing Multicore System Efficiency through Intelligent Bandwidth Shifting online bandwidth shifting mechanism; prefetch usefulness (PU) level 2015 HPCA IBM XChange: A Market-based Approach to Scalable Dynamic Multi-resource Allocation in Multicore Architectures CMP multiresource allocation mechanism XChange; market framework based modeling 2018 MICRO SNU RpStacks-MT: A High-throughput Design Evaluation Methodology for Multi-core Processors graph-based multi-core performance model; distance-based memory system model; dynamic scheduling reconstruction method 2023 MICRO THU MAICC: A Lightweight Many-core Architecture with In-Cache Computing for Multi-DNN Parallel Inference slice improved and hardware-implemented reduction CIM; ISA extension for CIM; CNN layer segmentation and mapping algorithm 2023 MICRO Yonsei McCore: A Holistic Management of High-Performance Heterogeneous Multicores cluster partitioning via index hash function; partitions balancing method; hardware support for RL based scheduling"},{"location":"hardware/accelerators/#application-optimization","title":"Application Optimization","text":"Year Venue Authors Title Tags P E N 2023 SC NUDT Optimizing Direct Convolutions on ARM Multi-Cores direct convolution algorithm NDirect; loop ordering algorithm; micro convolution kernal for computing &amp; packeting 2023 SC NUDT Optimizing MPI Collectives on Shared Memory Multi-Cores intra-node reduction algorithm for redundant data movements; fine grained non-temporal store based adaptive collectives 2024 PPoPP NUDT Towards Scalable Unstructured Mesh Computations on Shared Memory Many-Cores task dependency tree(TDT); tree traversal based parallel algorithm for CPU/GPU"},{"location":"hardware/accelerators/#heterogeneous-many-core-system","title":"Heterogeneous Many-core System","text":"Year Venue Authors Title Tags P E N 2018 ICCAD WSU Hybrid On-Chip Communication Architectures for Heterogeneous Manycore Systems many-to-few communication patterns; long range shortcut based wireless NoC ; 3D-TSV based heterogeneous NoC 2018 IEEE TC WSU On-Chip Communication Network for Efficient Training of Deep Convolutional Networks on Heterogeneous Manycore Systems wireless-enabled heterogeneous NoC; archived multi-objective simulated annealing for network connectivity"},{"location":"hardware/accelerators/#architecture-dse","title":"Architecture DSE","text":"<p>Challenge: It's crucial to find the optimal hardware configurations that meet performance; power; and area constraints for specific applications.</p>"},{"location":"hardware/accelerators/#mapping-co-exploration-dse","title":"Mapping &amp; Co-Exploration DSE","text":"<p>Challenge: Efficiently co-optimize DNN mapping and hardware architecture under complex constraints.</p> Year Venue Authors Title Tags P E N 2020 ICCAD UIUC DNNExplorer: A Framework for Modeling and Exploring a Novel Paradigm of FPGA-based DNN Accelerator two-level (global and local) automatic DSE engine; dynamic design space exploration framework; high-dimensional design space support 4 4 4 2024 HPCA THU Gemini: Mapping and Architecture Co-exploration for Large-scale DNN Chiplet Accelerators layer-centric encoding method; DP-based graph partition algorithm; SA based D2D link communication optimization 2024 ASPLOS THU Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication Optimization consumption-centric flow based subgraph execution scheme; main/side region based memory management 2024 ASPDAC CUHK SoC-Tuner: An Importance-guided Exploration Framework for DNN-targeting SoC Design intercluster distance algorithm; importance-based pruning and initialization 3 2 2"},{"location":"hardware/accelerators/#microarchitecture-cross-architecture-dse","title":"Microarchitecture &amp; Cross-Architecture DSE","text":"<p>Challenge: Efficiently explore and optimize design spaces across microarchitectures and heterogeneous hardware.</p> Year Venue Authors Title Tags P E N 2025 arXiv THU &amp; Macau MLDSE: Scaling Design Space Exploration Infrastructure for Multi-Level Hardware IR and builder based hardware modeling; cross-architecture DSE; spatial-level DSE 3 3 2 2025 arXiv PKU DiffuSE: Cross-Layer Design Space Exploration of DNN Accelerator via Diffusion-Driven Optimization diffusion-based design generation; conditional sampling 3 4 3"},{"location":"hardware/eda/","title":"Electronic Design Automation","text":""},{"location":"hardware/eda/#rtl-code-generation","title":"RTL Code Generation","text":"<p>Challenge: The need for automated RTL code generation tools to reduce the time and effort required for hardware design.</p> <p>Solution: Use advanced techniques such as LLM; graph-based approaches; and domain-specific languages to automate and optimize the RTL code generation process and integrate into existing design tools.</p> Year Venue Authors Title Tags P E N 2013 DAC Columbia A Method to Abstract RTL IP Blocks into C++ Code and Enable High-Level Synthesis process communication graph; I/O port loop unrolling; HLS design space expansion 2021 ASPLOS Cornell A compiler infrastructure for accelerator generators a split representation combining a high-level control flow language with a hardware-like structural language; pass-based compiler; systolic array generator; live-range-based register-sharing 4 3 3 2023 DATE NYU Benchmarking Large Language Models for Automated Verilog RTL Code Generation verilog code training corpus; multi-level verilog coding problems for analysis 2024 ISEDA UESTC GraphRTL: an Agile Design Framework of RTL Code from Data Flow Graphs graph error detection kernel; DFS based graph equivalent reconstruction; template/scala based DFG and CFG merging 2024 arXiv UCSD MAGE: A Multi-Agent Engine for Automated RTL Code Generation multi-agent; high-temperature sampling and ranking; verilog-state checkpoint debugging"},{"location":"hardware/emerging/","title":"Emerging Technologies","text":""},{"location":"hardware/emerging/#photonic-computing","title":"Photonic computing","text":"<p>Solution: Uses light (photons) instead of electricity (electrons) to perform calculations and process information. It leverages the unique properties of light; such as its speed and parallelism; to achieve high-performance computing.</p> Year Venue Authors Title Tags P E N 2021 Nature Swinburne 11 TOPS photonic convolutional accelerator for optical neural networks universal optical convolutional accelerator for vector processing 2025 arXiv ASU H3PIMAP: A Heterogeneity-Aware Multi-Objective DNN Mapping Framework on Electronic-Photonic Processing-in-Memory Architectures Electronic-Photonic-PIM Accelerator; coresponding mapping framework and evaluation infrastructure"},{"location":"hardware/fail/","title":"Security and Reliability","text":""},{"location":"hardware/fail/#error-pattern","title":"Error Pattern","text":""},{"location":"hardware/fail/#manycore-architecture","title":"Manycore Architecture","text":"Year Venue Authors Title Tags P E N 2009 MICRO UIUC mSWAT: Low-Cost Hardware Fault Detection and Diagnosis for Multicore Systems selective Triple Modular Redundant(TMR) replay method; symptom based fault detection; permanent/transient fault 2015 IEEE TSM NTU Wafer Map Failure Pattern Recognition and Similarity Ranking for Large-Scale Data Sets wafer map failure pattern; wafer map similarity ranking; radon/geometry-based feature extraction; WM-811K wafer map dataset"},{"location":"hardware/fail/#system-level","title":"System Level","text":"Year Venue Authors Title Tags P E N 2017 SC Argonne National Lab Run-to-run Variability on Xeon Phi based Cray XC Systems OS noise based core-level variability; tile-level varibility; memory mode varibility 2018 FAST UChicago Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems conversion among fail-stop/slow/trasient; permanent/transient/partial slowdown; internal/external root causes"},{"location":"hardware/fail/#hardware-fault","title":"Hardware Fault","text":"Year Venue Authors Title Tags P E N 2014 DTIS LIRMM A Survey on Simulation-Based Fault Injection Tools for Complex Systems runtime fault injection; compile-time fault injection 2021 ASPLOS UIUC BayesPerf: Minimizing Performance Monitoring Errors using Bayesian Statistics microarchitectural relationship incorporation; measurement uncertainty quantification; high-frequency sampling reduction 3 4 3 2024 arXiv GWU Algorithmic Strategies for Sustainable Reuse of Neural Network Accelerators with Permanent Faults stack-at-0/1 faults; weight register fault; invertible scaling and shifting technique; elementary tile operations for mantissa fault 2025 arXiv NUDT FlexStep: Enabling Flexible Error Detection in Multi/Many-core Real-time Systems register checkpoints based error detection; memory access log unit; data buffering and channelling unit 2025 DAC SEU MEEK: Re-thinking Heterogeneous Parallel Error Detection Architecture for Real-World OoO Superscalar Processors data extraction unit; bespoke forwarding fabric; little core upgrade 3 4 3"},{"location":"hardware/fail/#noc-fault","title":"NoC Fault","text":"Year Venue Authors Title Tags P E N 2006 IOLTS UBC &amp; WSU On-line Fault Detection and Location for NoC Interconnects code-disjoint based error detection algorithm; code-disjoint switch design 2 2 2 2011 ASPDAC NTHU On the Design and Analysis of Fault Tolerant NoC Architecture Using Spare Routers shift-and-replace allocation algorithm; defect-awareness-path allocation algorithm 3 2 2 2013 TVLSI NUDT Addressing Transient and Permanent Faults in NoC With Efficient Fault-Tolerant Deflection Router link-level error control scheme; on-line fault diagnosis mechanism;RL based fault-tolerant deflection routing 4 2 2 2017 TECS NTUA SoftRM: Self-Organized Fault-Tolerant Resource Management for Failure Detection and Recovery in NoC Based Many-Cores permanent fault; tweaked perfect failure detector; paxos algorithm to recover fault 2 4 2 2017 DDECS TTU From Online Fault Detection to Fault Management in Network-on-Chips: A Ground-Up Approach data-path fault detection; control part fault detection; assertion vector based fault localization 3 1 2"},{"location":"hardware/fail/#fail-slow","title":"Fail-Slow","text":"<p>Challenge: Fail-slow faults can cause performance degradation without complete failure; making them difficult to detect and diagnose than the fail-stop failure.</p> Year Venue Authors Title Tags P E N 2019 ATC UChicago IASO: A Fail-Slow Detection and Mitigation Framework for Distributed Storage Services slowdown detection based on peer score; sub-root causes for five kinds of root causes 2022 ATC SJTU &amp; Alibaba NVMe SSD Failures in the Field: the Fail-Stop and the Fail-Slow hardware infant mortality; write amplification factor; intra-node/rock failure 3 4 2 2023 FAST SJTU &amp; Alibaba PERSEUS: A Fail-Slow Detection Framework for Cloud Storage Systems outlier data detection; regression model for detection threshold; risk evaluating algorithm 4 4 3 2025 ASPDAC Xiamen University A Fail-Slow Detection Framework for HBM Devices outlier data detection; regression model for detection threshold; risk evaluating algorithm 2 4 2"},{"location":"hardware/fail/#physical-effects","title":"Physical Effects","text":""},{"location":"hardware/fail/#rram","title":"RRAM","text":"<p>Challenge: Non-ideal effects of RRAM devices (e.g. device-to-device variation; cycle-to-cycle variation; etc.) can cause significant performance degradation.</p> <p>Solution: Data types; training algorithm; SRAM for compensation.</p> Year Venue Authors Title Tags P E N 2019 DAC UCF Noise Injection Adaption: End-to-End ReRAM Crossbar Non-ideal Effect Adaption for Neural Network Mapping stuck-at-fault; crossbar wire resistance based IR drop; thermal noise model; shot noise; random telegraph noise 2019 DATE Georgia Tech Design of Reliable DNN Accelerator with Un-reliable ReRAM dynamical fixed point data representation format; device variation aware training methodology 2020 DAC ASU Accurate Inference with Inaccurate RRAM Devices: Statistical Data, Model Transfer, and On-line Adaptation introduce statistical variations in knowledge distillation; On-line sparse adaptation with a small SRAM array 2020 DATE SJTU Go Unary: A Novel Synapse Coding and Mapping Scheme for Reliable ReRAM-based Neuromorphic Computing unary coding; priority mapping* 2022 TCAD ASU Hybrid RRAM/SRAM in-Memory Computing for Robust DNN Acceleration integrates an RRAM-based IMC macro with a digital SRAM macro using a programmable shifter to compensate for RRAM variations; ensemble learning 2023 ISCAS TAMU Memristor-based Offset Cancellation Technique in Analog Crossbars peripheral circuitry to remove the systematic offset of crossbar 2024 LATS AMU Analysis of Conductance Variability in RRAM for Accurate Neuromorphic Computing analyzation and quantification of conductance variability in RRAMs; analysis of conductance variation over multiple cycles 2025 arXiv AMU Energy-Efficient RRAM-Based Neuromorphic Computing with Adaptive Voltage and Frequency Scaling energy-efficient RRAM-based neuromorphic computing; adaptive voltage and frequency scaling; energy-efficient RRAM-based neuromorphic computing 2 4 3"},{"location":"hardware/fail/#dram","title":"DRAM","text":"<p>Challenge: DRAM devices are sensitive to temperature and voltage variations; which can lead to performance degradation and reliability issues.</p> Year Venue Authors Title Tags P E N 2015 RACS NTU Thermal/Performance Characterization of CMPs with 3D-stacked DRAMs under Synergistic Voltage-Frequency Control of Cores and DRAMs coordinate dynamic voltage and frequency scaling; thermal efficiency quantification 3 2 2 2017 IEEE Access Yuan Ze University Thermal- and Performance-Aware Address Mapping for the Multi-Channel Three-Dimensional DRAM Systems inter-channel bank swapping; inter-channel bank reordering 3 3 2 2020 TCAD BUAA Temperature-Aware DRAM Cache Management\u2014Relaxing Thermal Constraints in 3-D Systems temperature-safe cache operation; exploration on cache remapping; write-back optimization 4 3 2 2024 TCAD IIT 3D-TemPo: Optimizing 3-D DRAM Performance Under Temperature and Power Constraints reward-based dynamic power budgeting; adjacency awareness; DRAM low-power-based DTM 3 3 2"},{"location":"hardware/fail/#3dic","title":"3DIC","text":"Year Venue Authors Title Tags P E N 2004 ICCAD UCLA A thermal-driven floorplanning algorithm for 3D ICs combined bucket and 2D array; tile stack based model; horizontal and vertical heat flow analysis 2016 IJHMT UCR Analysis of critical thermal issues in 3D integrated circuits thermal hotspots; impact of thermal interface materials; power distribution; processor pitch and area"},{"location":"hardware/fail/#fault-tolerant-cache","title":"Fault-Tolerant Cache","text":"Year Venue Authors Title Tags P E N 2009 ICCD NUS The Salvage Cache: A fault-tolerant cache architecture for next-generation memory technologies fault-bit protection for divisions; victim map based division replacement 2011 CASES UCSD FFT-Cache: A Flexible Fault-Tolerant Cache Architecture for Ultra Low Voltage Operation flexible defect map for faulty block; FDM configuration algorithm; non-functional lines minimization"},{"location":"hardware/memory/","title":"Memory Architecture","text":""},{"location":"hardware/memory/#ndp-ssds","title":"NDP: SSDs","text":"<p>Solution: Intergrate the compute unit into the SSD controller to process the capacity-sensitive applications.</p> Year Venue Authors Title Tags P E N 2024 HPCA UCLA BeaconGNN: Large-Scale GNN Acceleration with Out-of-Order Streaming In-Storage Computing DirectGraph format for out-of-order sampling; die-level processing units; channel-level command router 4 2 3 2025 ISCA ETHZ REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing In-Storage processing 2 4 3 2025 ISCA UCSD In-Storage Acceleration of Retrieval Augmented Generation as a Service metamorphic in-storage accelerator; Metadata Navigation Unit for dynamic data access 4 3 2"},{"location":"hardware/memory/#ndp-dimm","title":"NDP: DIMM","text":"<p>Challenge: Memory wall causing high latency of data transfer between CPU and memory.</p> <p>Solution: Put the compute unit in the memory or near the memory to reduce the data transfer overhead.</p>"},{"location":"hardware/memory/#application-specific-optimization","title":"Application-Specific Optimization","text":"<p>Challenge: Existing NDP architecture are designed for general-purpose computing; not efficient for specific tasks like LLM.</p> Year Venue Authors Title Tags P E N 2022 ISCA Micron To PIM or Not for Emerging General Purpose Processing in DDR Memory Systems vector engine inside NDP bank; intelligent code offload decision 2 3 2 2024 ISCA Samsung pSyncPIM: Partially Synchronous Execution of Sparse Matrix Operations for All-Bank PIM Architectures partially synchronous PIM control; predicated execution; sparse matrix distribution &amp; compaction; 3 3 3 2024 npj Unconv. Comput. UMich PIM-GPT: a hybrid process in memory accelerator for autoregressive transformers hybrid system to accelerate GPT inference; mapping scheme for data locality and workload distribution 3 2 2 2025 arxiv ETHZ MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem PIM module inside the SSD controller; early signal quantization; read filtering 3 3 2 2025 HPCA UC Davis NOVA: A Novel Vertex Management Architecture for Scalable Graph Processing message-driven processors capable of executing algorithms; a direct-mapped cache with a write-back policy; support both asynchronous and bulk synchronous parallel execution models 3 3 3"},{"location":"hardware/memory/#memory-allocation-management","title":"Memory Allocation &amp; Management","text":"<p>Challenge: Existing NDP architecture has numerous independent memory spaces; lacks unified management; and features inefficient memory allocation.</p> Year Venue Authors Title Tags P E N 2024 ISCA SJTU UM-PIM: DRAM-based PIM with Uniform &amp; Shared Memory Space Uniform shared CPU-PIM memory; dual-track memory management; zero-copy data re-layout 3 3 4 2024 ISCA KAIST PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for Processing-In-Memory (PIM) Architectures PIM-specific memory allocator; hierarchical memory allocation scheme; hardware metadata cache 4 2 3 2024 arXiv ETHZ PUMA: Efficient and Low-Cost Memory Allocation and Alignment Support for Processing-Using-Memory Architectures aligned memory allocator for PUM; DRAM-aware memory allocation 2 3 2 2024 MICRO KAIST PIM-MMU: A Memory Management Unit for Accelerating Data Transfers in Commercial PIM Systems data copy engine for host-PIM transfers; PIM-aware memory scheduler for MLP maximization; memory remapping unit for dual address mapping 2 4 3"},{"location":"hardware/memory/#pim-compiler-isa-extension","title":"PIM Compiler &amp; ISA Extension","text":"<p>Challenge: Existing compilers are not optimized for locality-aware PIM architectures and require specialized programming models to fully utilize PIM capabilities.</p> Year Venue Authors Title Tags P E N 2015 ISCA Seoul National PIM-Enabled Instructions: A Low-Overhead; Locality-Aware Processing-in-Memory Architecture PIM-Enabled Instructions for ISA extension; PIM directory for atomicity and coherence; single-cache-block restriction 3 4 4 2020 ISCA UCSB iPIM: Programmable In-Memory Image Processing Accelerator Using Near-Bank Architecture Single-Instruction-Multiple-Bank ISA; register allocation; instruction reordering 4 4 2 2025 ISCA POSTECH ATIM: Autotuning Tensor Programs for Processing-in-DRAM autotuning framework for DRAM PIM; search-based optimizing tensor compiler; balanced evolutionary search algorithm 3 3 4"},{"location":"hardware/memory/#evaluation-simulators","title":"Evaluation &amp; Simulators","text":"Year Venue Authors Title Tags P E N 2025 HPCA THU UniNDP: A Unified Compilation and Simulation Tool for Near DRAM Processing Architectures unified NDP hardware abstraction; NDP compiler optimization; instruction-driven NDP simulator 3 5 2 2025 arXiv ETHZ EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques FPGA-based DRAM evaluation framework; C++ high-level language for description; time scaling for accurate modeling 3 4 3"},{"location":"hardware/memory/#intra-dimm-communication","title":"Intra-DIMM Communication","text":"<p>Challenge: High latency of intra-DIMM (cross-bank) communication via host CPU forwarding.</p> Year Venue Authors Title Tags P E N 2024 ISCA THU NDPBridge: Enabling Cross-Bank Coordination in Near-DRAM-Bank Processing Architectures gather &amp; scatter messages via buffer chip; task-based message-passing model; hierarchical, data-transfer-aware load balancing 2025 HPCA Samsung Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory Scatter-Gather In-DRAM fine-grained scatter-gather via data bus offsets; fine-grained cache architecture using fg-tags; Standard DDR command interpretation for FIM control; Combined graph tiling with fine-grained memory access 3 3 4 2025 arXiv ETHZ PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System PIMDAL library for DB operators; quicksort/mergesort/hashing on UPMEM PIM; scatter/gather/async transfers for PIM communication 4 4 2 2024 arXiv Seoul National PID-Comm: A Fast and Flexible Collective Communication Framework for Commodity Processing-in-DIMM Devices Virtual hypercube PIM model; PE-assisted data reordering; in-register and cross-domain data modulation 3 4 3 2025 ISCA KAIST PIMnet: A Domain-Specific Network for Efficient Collective Communication in Scalable PIM domain-specific PIM interconnect; hierarchical network for PIM packaging; PIM-controlled deterministic scheduling 2 4 3"},{"location":"hardware/memory/#inter-dimm-communication","title":"Inter-DIMM Communication","text":"<p>Challenge: High latency of inter-DIMM (cross-DIMM) communication via host CPU forwarding.</p> Year Venue Authors Title Tags P E N 2017 MEMSYS UCLA AIM: Accelerating Computational Genomics through Scalable and Noninvasive Accelerator-Interposed Memory placing FPGA chip between DIMM and the conventional memory network; multi-drop bus for inter-accelerator communication 1 2 2 2023 ASPLOS THU ABNDP: Co-optimizing Data Access and Load Balance in Near-Data Processing Traveller Cache; hybrid task scheduling; hybrid scheduling leveraging distributed cache 4 3 4 2023 HPCA PKU DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing high-speed hardware link bridges between DIMMs; direct intra-group P2P communication &amp; broadcast; hybrid routing mechanism for inter-group communication 2025 HPCA SJTU AsyncDIMM: Achieving Asynchronous Execution in DIMM-Based Near-Memory Processing Offload-Schedule-Return mechanism; switch-recovery scheduling; explicit/implicit synchronization 2 4 3 2018 MICRO UIUC Application-Transparent Near-Memory Processing Architecture with Memory Channel Network integrates a processor on a buffered DIMM; application-transparent near-memory processing; leverages memory channels for high-bandwidth/low-latency inter-processor communication 3 4 4"},{"location":"hardware/memory/#concurrent-host-and-pim-operations","title":"Concurrent Host and PIM operations","text":"<p>Challenge: High latency of concurrent host CPU/GPU and PIM operations via host CPU forwarding.</p> Year Venue Authors Title Tags P E N 2024 IEEE CA KAIST Analysis of Data Transfer Bottlenecks in Commercial PIM Systems: A Study With UPMEM-PIM runtime data transposition causing high CPU overhead; PIM-integrated system memory mapping impact 2 2 2 2024 ASPLOS KAIST NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing dual row buffer architecture; sub-batch interleaving; greedy min-load bin packing algorithm 3 4 3 2025 HPCA ICT Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM activation sparsity-based hot(GPU)/cold(NDP) neuron partitioning; offline ILP + online predictor for neuron partition; window-based online remapping for GPU-NDP &amp; NDP-NDP load balance 2 3 4 2025 ISCA Univ. of Virginia Membrane: Accelerating Database Analytics with Bank-Level DRAM-PIM Filtering bank-level DRAM-PIM filtering; CPU-PIM cooperative query execution; denormalization for PIM-amenable filtering 3 3 2"},{"location":"hardware/memory/#optimizations-on-upmem-pim","title":"Optimizations on UPMEM-PIM","text":"<p>Challenge: The original UMPEM API library is not well-suited for all workloads especially for those with cross-bank communication.</p> Year Venue Authors Title Tags P E N 2023 arXiv ETHZ A Framework for High-throughput Sequence Alignment using Real Processing-in-Memory Systems Alignment-in-Memory framework; hybrid WRAM-MRAM sketch data management for PIM 2 3 4 2025 arXiv ETHZ PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System PIMDAL library on UPMEM PIM system for data analytics; scatter/gather-aware transfers for inter-PIM communication; Apache Arrow for host memory management 3 3 3"},{"location":"hardware/memory/#pim-in-cache-computing","title":"PIM: In-Cache-Computing","text":"Year Venue Authors Title Tags P E N 2025 arXiv Torino ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions ARCANE in-cache NMC coprocessor architecture; software-defined matrix ISA for NMC abstraction; cache-integrated control runtime for NMC management 3 4 4"},{"location":"hardware/memory/#pim-ndp-benchmarks","title":"PIM &amp; NDP: Benchmarks","text":"<p>Challenge: Conventional parallel computing benchmarks are not suitable for PIM/NDP.</p>"},{"location":"hardware/memory/#benchmarks-for-conventional-computing","title":"Benchmarks for Conventional Computing","text":"Year Venue Authors Title Tags P E N 2021 ATC UBC A Case Study of Processing-in-Memory in off-the-Shelf Systems benchmark 2022 IEEE Access ETH Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System benchmark suite PrIM 2024 CAL KAIST Analysis of Data Transfer Bottlenecks in Commercial PIM Systems: A Study With UPMEM-PIM low MLP; manual data placement; unbalanced thread allocation and scheduling 2024 IEEE Access Lisbon NDPmulator: Enabling Full-System Simulation for Near-Data Accelerators From Caches to DRAM simulator PiMulator based on Ramulator &amp; gem5; full system support; multiple ISA support 2024 HPCA KAIST Pathfinding Future PIM Architectures by Demystifying a Commercial PIM Technology simulator uPIMulator"},{"location":"hardware/memory/#benchmarks-for-quantum-computing","title":"Benchmarks for Quantum Computing","text":"Year Venue Authors Title Tags P E N 2025 ASPDAC NUS PIMutation: Exploring the Potential of PIM Architecture for Quantum Circuit Simulation PIMutation framework for quantum circuit simulation; gate merging optimization; row swapping instead of matrix multiplication; vector partitioning for separable states; leveraging UPMEM PIM architecture"},{"location":"hardware/memory/#ndp-cxl","title":"NDP: CXL","text":"<p>Challenge: No direct physical connectivity between the banks in the DIMM-based NDP architecture. Limited number of DDR channels causing poor scalability.</p> <p>Solution: Introduce CXL-based interconnects to enable direct communication between memory banks; Use CXL memory pools and CXL switches to enable scalable NDP architecture.</p> Year Venue Authors Title Tags P E N 2022 MICRO UCSB BEACON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support scalable hardware accelerator inside CXL switch or bank; lossless memory expansion for CXL memory pools 2024 ICS Samsung CLAY: CXL-based Scalable NDP Architecture Accelerating Embedding Layers direct interconnect between DRAM clusters; dedicated memory address mapping scheme; Multi-CLAY system support through customized CXL switch 2024 MICRO SK Hyrix Low-overhead General-purpose Near-Data Processing in CXL Memory Expanders CXL.mem protocol instead of CXL.io (DMA) for low-latency; lightweight threads to reduce address calculation overhead 2025 ISCA Seoul National COSMOS: A CXL-Based Full In-Memory System for Approximate Nearest Neighbor Search CXL core-based ANNS task offload; rank-level parallel distance computation; adjacency-aware data placement algorithm 2 2 2 2025 ASPLOS UMich PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference hierarchical CXL PIM-PNM compute architecture; use die-shot to estimate area cost; multiple LLM parallelism policies 2 3 3"},{"location":"hardware/memory/#ndp-3d-stacked-dram","title":"NDP: 3D-stacked DRAM","text":"<p>Challenge: No direct physical connectivity between the banks in the DIMM-based NDP architecture.</p> <p>Solution: Use TSVs to provide TB/s level bandwidth in inter-bank communication &amp; band-to-logic layer communication.</p> Year Venue Authors Title Tags P E N 2013 PACT KAIST Memory-centric System Interconnect Design with Hybrid Memory Cubes memory-centric network; distributor-based topology for reduced latency; non-minimal routing for higher throughput 2024 DAC SNU MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models NDP for MoE; activation movement; GPU-MoNDE load-balancing scheme 2024 ASPLOS PKU SpecPIM: Accelerating Speculative Inference on PIM-Enabled System via Architecture-Dataflow Co-Exploration algorithmic and architectural heterogeneity; PIM resource allocation; multi-model collaboration workflow"},{"location":"hardware/memory/#benchmark","title":"Benchmark","text":"Year Venue Authors Title Tags P E N 2019 DAC ETHZ NAPEL: Near-Memory Computing Application Performance Prediction via Ensemble Learning simulator Ramulator-PIM; tracefile from Ramulator &amp; run on zsim 2021 CAL UVA MultiPIM: A Detailed and Configurable Multi-Stack Processing-In-Memory Simulator simulator MultiPIM; multi-stack &amp; virtual memory support; parallel offloading"},{"location":"hardware/memory/#ndp-heterogeneous-architecture","title":"NDP: Heterogeneous Architecture","text":"<p>Challenge: Different PIM architectures have different characteristics and performance trade-offs; communicating between different PIM architectures is challenging.</p> Year Venue Authors Title Tags P E N 2025 ISCA HUST HeterRAG: Heterogeneous Processing-in-Memory Acceleration for Retrieval-augmented Generation combine DIMM-PIM and HBM-PIM for acceleration; locality-aware retrieval and generation; fine-grained parallel pipelining 2 3 3"},{"location":"hardware/memory/#general-cim","title":"General CiM","text":""},{"location":"hardware/memory/#specific-application-algorithm","title":"Specific Application &amp; Algorithm","text":"Year Venue Authors Title Tags P E N 2024 ISVLSI USC Multi-Objective Neural Architecture Search for In-Memory Computing neural architecture search methodology; integration of Hyperopt, PyTorch and MNSIM 2024 arXiv Intel CiMNet: Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware framework that jointly searches for optimal sub-networks and hardware configurations for CiM architectures; multi-objective evolutionary search method 4 2 4 2025 AICAS UVA Optimizing and Exploring System Performance in Compact Processing-in-Memory-based Chips Pipeline Method for Compact PIM Designs; Dynamic Duplication Method (DDM); Maximum NN Size Estimation &amp; Deployment in Compact PIM Design"},{"location":"hardware/memory/#modeling-simulation","title":"Modeling &amp; Simulation","text":"Year Venue Authors Title Tags P E N 2018 TCAD ASU NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning estimate the circuit-level performance of neuro-inspired architectures; estimates the area, latency, dynamic energy, and leakage power; Support both SRAM and eNVM; tested on 2-layer MLP NN, MNIST 2019 IEDM Georgia Tech DNN+NeuroSim: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators with Versatile Device Technologies a python wrapper to interface NeuroSim; for inference only 2020 TCAD ZJU Eva-CiM: A System-Level Performance and Energy Evaluation Framework for Computing-in-Memory Architectures models for capturing memory access and dependency-aware ISA traces; models for quantifying interactions between the host CPU and the CiM module 2024 ISPASS MIT CiMLoop: A Flexible, Accurate, and Fast Compute-In-Memory Modeling Tool flexible specification to describe CiM systems; accurate model/fast statistical model of data-value-dependent component energy 2025 ASPDAC HKUST MICSim: A Modular Simulator for Mixed-signal Compute-in-Memory based AI Accelerator modulared Neurosim; data statistic-based average-mode instead of trace-based mode 4 3 2"},{"location":"hardware/memory/#cim-dram","title":"CIM: DRAM","text":"<p>Solution: Rather than placing logic units into DRAM; modify the physical structure of DRAM/eDRAM to enable in-memory computing.</p> Year Venue Authors Title Tags P E N 2021 ICCD ASU CIDAN: Computing in DRAM with Artificial Neurons Threshold Logic Processing Element (TLPE) for in-memory computation; Four-bank activation window; Configurable threshold functions; Energy-efficient bitwise operations; Integration with DRAM architecture 2022 HPCA UCSD TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer token-based dataflow for general Transformer-based models; ring-based data broadcast in modified HBM 4 2 4 2024 A-SSCC UNIST A 273.48 TOPS/W and 1.58 Mb/mm2 Analog-Digital Hybrid CIM Processor with Transpose Ternary-eDRAM Bitcell analog DRAM CIM for partial sum and digital adder 1 4 2 2025 arXiv KAIST RED: Energy Optimization Framework for eDRAM-based PIM with Reconfigurable Voltage Swing and Retention-aware Scheduling RED framework for energy optimization; reconfigurable eDRAM design; retention-aware scheduling; trade-off analysis between RBL voltage swing, sense amplifier power, and retention time; refresh skipping and sense amplifier power gating 2025 arXiv UTokyo MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration GeMV operations for end-to-end low-bit LLM inference using unmodified DRAM; processor-DRAM co-design; on-the-fly vector encoding; horizontal matrix layout 4 4 3"},{"location":"hardware/memory/#cim-sram","title":"CIM: SRAM","text":"<p>Challenge: Memory wall causing high latency of data transfer between CPU and memory; DIMM-based NDP causing high energy consumption; area overhead and low performance efficiency.</p> <p>Solution: Generally modify the physical structure of SRAM to enable in-memory computing; rather than placing logic units into SRAM.</p>"},{"location":"hardware/memory/#sram-cim-general-architecture","title":"SRAM CIM: General Architecture","text":"Year Venue Authors Title Tags P E N 2024 TCASAI Purdue Algorithm Hardware Co-Design for ADC-Less Compute In-Memory Accelerator reduce ADC overhead in analog CiM architectures; Quantization-Aware Training; Partial Sum Quantization; ADC-Less hybrid analog-digital CiM hardware architecture HCiM 2024 ISCAS NYCU CIMR-V: An End-to-End SRAM-based CIM Accelerator with RISC-V for AI Edge Device incorporates CIM layer fusion, convolution/max pooling pipeline, and weight fusion; weight fusion: pipelining the CIM convolution and weight loading 2018 JSSC MIT CONV-SRAM: An Energy-Efficient SRAM With In-Memory Dot-Product Computation for Low-Power Convolutional Neural Networks SRAM-embedded convolution (dot-product) computation architecture for BNN; support multi-bit input-output 2022 TCAD NTHU MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with Co-designed Compressed Neural Networks sparsity algorithm designed for SRAM CiM; quantization algorithm with BN fusion 2024 ESSCIRC THU A 65nm 8b-Activation 8b-Weight SRAM-Based Charge-Domain Computing-in-Memory Macro Using A Fully-Parallel Analog Adder Network and A Single-ADC Interface SRAM-based CD-CiM architecture; charge-domain analog adder tree; ReLU-optimized ADC 4 4 4 2021 ISSCC TSMC An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In Memory Macro in 22nm for Machine-Learning Edge Applications programmable bit-widths for both input and weights; SRAM and CIM mode 2 5 1"},{"location":"hardware/memory/#sram-cim-specific-application","title":"SRAM CIM: Specific Application","text":"Year Venue Authors Title Tags P E N 2023 TCAS-I UIC MC-CIM: Compute-in-Memory With Monte-Carlo Dropouts for Bayesian Edge Intelligence SRAM-based CIM macros to accelerate Monte-Carlo dropout; compute reuse between consecutive iterations"},{"location":"hardware/memory/#sram-cim-simulator-modeling","title":"SRAM CIM: Simulator &amp; Modeling","text":"Year Venue Authors Title Tags P E N 2020 ISCAS JCU MemTorch: A Simulation Framework for Deep Memristive Cross-Bar Architectures supports both GPUs and CPUs; integrates directly with PyTorch; simulate non-idealities of memristive devices within cross-bar, tested on VGG-16, CIFAR-10 2021 TCAD Geogia Tech DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for On-Chip Training non-ideal device properties of NVMS' effect for on-chip training"},{"location":"hardware/memory/#sram-cim-transformer-accelerator","title":"SRAM CIM: Transformer Accelerator","text":"<p>Challenge: Transformer architecture is widely used in NLP and CV tasks. Existing SRAM CIM architectures are not suitable for transformer acceleration.</p> Year Venue Authors Title Tags P E N 2025 DATE PKU Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs architecture model and simulator for CIM-based TPUs; designed for LLM inference 4 2 4 2023 arXiv Keio An 818-TOPS/W CSNR-31dB SQNR-45dB 10-bit Capacitor-Reconfiguring Computing-in-Memory Macro with Software-Analog Co-Design for Transformers Capacitor-Reconfiguring analog CIM architecture 1 4 3 2025 arXiv Purdue Hardware-Software Co-Design for Accelerating Transformer Inference Leveraging Compute-in-Memory SRAM based softmax-friendly CIM architecture for transformer; finer-granularity pipelining strategy 4 3 2 2025 arXiv PKU Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs Energy-efficient CIM core integration in TPUs (replace the original MXU); CIM-MXU with systolic data path; Array dimension scaling for CIM-MXU; Area-efficient CIM macro design; Mapping engine for generative model inference 2024 JSSC THU MulTCIM: Digital Computing-in-Memory-Based Multimodal Transformer Accelerator With Attention-Token-Bit Hybrid Sparsity long reuse elimination scheduler (LRES) to dynamically reshape the attention matrix; runtime token pruner (RTP) to remove insignificant tokens; modal-adaptive CIM network (MACN) to dynamically divide CIM cores into Pipeline; effective-bits-balanced CIM (EBBCIM) macro architecture 5 4 3"},{"location":"hardware/memory/#cim-rram","title":"CIM: RRAM","text":"<p>Challenge: RRAM devices are non-volatile and have high density; suitable for CIM applications. However; RRAM devices have non-ideal effects that can cause significant performance degradation.</p>"},{"location":"hardware/memory/#rram-cim-simulator","title":"RRAM CiM: Simulator","text":"Year Venue Authors Title Tags P E N 2018 TCAD THU MNSIM: Simulation Platform for Memristor-Based Neuromorphic Computing System reference design for largescale neuromorphic accelerator and can also be customized; behavior-level computing accuracy model 2023 TCAD THU MNSIM 2.0: A Behavior-Level Modeling Tool for Processing-In-Memory Architectures integrated PIM-oriented NN model training and quantization flow; unified PIM memory array model; support for mixed-precision NN operations 2024 DATE UCAS PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators event-driven simulation approach; can evaluate the optimizations of software and hardware independently"},{"location":"hardware/memory/#rram-cim-architecture","title":"RRAM CiM: Architecture","text":"Year Venue Authors Title Tags P E N 2019 ASPLOS Purdue &amp; HP PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference Programmable and general-purpose ReRAM based ML Accelerator; Supports an instruction set; Has potential for DNN training; Provides simulator that accepts model 2018 ICRC Purdue &amp; HP Hardware-Software Co-Design for an Analog-Digital Accelerator for Machine Learning compiler to translate model to ISA; ONNX interpreter to support models in common DL frame work; simulator to evaluate performance 2023 NANOARCH HUST Heterogeneous Instruction Set Architecture for RRAM-enabled In-memory Computing General ISA for RRAM CiM &amp; digital heterogeneous architecture; a tile-processing unit-array three-level architecture 2024 VLSI-SoC RWTH Aachen University Architecture-Compiler Co-design for ReRAM-Based Multi-core CIM Architectures inference latency predictions and analysis of the crossbar utilization for CNN 2024 arXiv CAS A Fully Hardware Implemented Accelerator Design in ReRAM Analog Computing without ADCs Based on Stochastic Binary Neural Networks; Winner-Take-All (WTA) strategy; Hardware implemented sigmoid and softmax 4 3 4"},{"location":"hardware/memory/#rram-cim-architecture-optimization","title":"RRAM CiM: Architecture optimization","text":"Year Venue Authors Title Tags P E N 2024 MICRO HUST DRCTL: A Disorder-Resistant Computation  Translation Layer Enhancing the Lifetime and  Performance of Memristive CIM Architecture address conversion method for dynamic scheduling; hierarchical wear-leveling (HWL) strategy for reliability improvement; data layout-aware selective remapping (LASR) to improve communication locality and reduce latency 2024 DATE RWTH Aachen University CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory Architectures algorithm to decide which parts of NN are duplicated to reduce inference latency; cross layer scheduling on tiled CIM architectures 2024 TC SJTU ERA-BS: Boosting the Efficiency of ReRAM-Based  PIM Accelerator With Fine-Grained  Bit-Level Sparsity bit-level sparsity in both weights and activations; bit-flip scheme; dynamic activation sparsity exploitation scheme 2023 TETCI TU Delft Accurate and Energy-Efficient Bit-Slicing for RRAM-Based Neural Networks unbalanced bit-slicing scheme for higher accuracy; holistic solution using 2's compliment 2024 Science USC Programming memristor arrays with arbitrarily high precision for analog computing represent high-precision numbers using multiple relatively low-precision analog devices;using RRAM CIM to solve PDEs 5 4 3"},{"location":"hardware/memory/#rram-cim-modeling","title":"RRAM CiM: Modeling","text":"Year Venue Authors Title Tags P E N 2024 AICAS RWTH Aachen University A Calibratable Model for Fast Energy Estimation of MVM Operations on RRAM Crossbars system energy model for MVM on ReRAM crossbars; methodology to study the effect of the selection transistor and wire parasitics in 1T1R crossbar arrays 2024 arXiv MIT Modeling Analog-Digital-Converter Energy and Area for Compute-In-Memory Accelerator Design architecture-level model that estimates ADC energy and area 4 3 3"},{"location":"hardware/memory/#rram-cim-training-optimization","title":"RRAM CiM: Training optimization","text":"Year Venue Authors Title Tags P E N 2021 TCAD SJTU ITT-RNA: Imperfection Tolerable Training for RRAM-Crossbar-Based Deep Neural-Network Accelerator prevent the large-weight synapses from being mapped to the imperfect memristor cells; off-device training algorithm to alleviate the accumulation of errors across multiple layers; bit-wise mechanism to compensate the resistance variations 3 3 2 2023 arXiv UND U-SWIM: Universal Selective Write-Verify for Computing-in-Memory Neural Accelerators only do write-verify for important weights; based on weight second derivatives as a guide 3 3 3 2023 Adv. Mater. UMich Bulk\u2010Switching Memristor\u2010Based Compute\u2010In\u2010Memory Module for Deep Neural Network Training Bulk-ReRAM based digital-CIM hybrid architecture for training; CIM for forward, digital for backward 4 4 1 2024 APIN SWU Multi-optimization scheme for in-situ training of memristor neural network based on contrastive learning optimizations to the deployment method, loss function and gradient calculation; compensation measures for non-ideal effects 2025 TNNLS SNU Efficient Hybrid Training Method for Neuromorphic Hardware Using Analog Nonvolatile Memory Hybrid offline-online training method"},{"location":"hardware/memory/#rram-cim-compiler","title":"RRAM CiM: Compiler","text":"<p>Challenge: Compiler for RRAM CIM is not well studied. Existing compilers are either for specific architecture or not efficient.</p> Year Venue Authors Title Tags P E N 2023 TACO HUST A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures compilation tool to migrate legacy programs to CPU/CIM heterogeneous architectures; a model to quantify the performance gain 2023 DAC CAS PIMCOMP: A Universal Compilation Framework for Crossbar-based PIM DNN Accelerators compiler based on Crossbar/IMA/Tile/Chip hierarchy; low latency and high throughput mode; genetic algorithm to optimize weight replication and core mapping; scheduling algorithms for complex DNN 2024 ASPLOS CAS CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory Accelerators compilation stack for various CIM accelerators; multi-level DNN scheduling approach"},{"location":"hardware/memory/#rram-cim-float-point-processing","title":"RRAM CiM: Float-Point processing","text":"<p>Challenge: Raw RRAM devices are not suitable for floating-point operations; while floating point data is common in DNNs (e.g. FP32).</p> Year Venue Authors Title Tags P E N 2023 SC UCLA ReFloat: Low-Cost Floating-Point Processing in ReRAM for Accelerating Iterative Linear Solvers data format and accelerator architecture 2024 DATE UESTC AFPR-CIM: An Analog-Domain Floating-Point RRAM -based Compute- In- Memory Architecture with Dynamic Range Adaptive FP-ADC all-analog domain CIM architecture for FP8 calculations; adaptive dynamic range FP-ADC &amp; FP-DAC 2025 arXiv GWU A Hybrid-Domain Floating-Point Compute-in-Memory Architecture for Efficient Acceleration of High-Precision Deep Neural Networks SRAM based hybrid-domain FP CIM architecture; detailed circuit schematics and physical layouts"},{"location":"hardware/memory/#rram-cim-convolutional-layer","title":"RRAM CiM: Convolutional Layer","text":"<p>Challenge: Convolutional layer is the most compute-intensive layer in CNNs. RRAM CIM architecture is quite suitable for convolutional layer operations but face challenges related to non-ideal effects and performance degradation.</p> Year Venue Authors Title Tags P E N 2020 Nature THU Fully hardware-implemented memristor convolutional neural network fabrication of high-yield, high-performance and uniform memristor crossbar arrays; hybrid-training method; replication of multiple identical kernels for processing different inputs in parallel 2020 TCAS-I Georgia Tech Optimizing Weight Mapping and Data Flow for Convolutional Neural Networks on Processing-in-Memory Architectures weight mapping to avoid multiple access to input; pipeline architecture for conv layer calculation 2019 TED PKU Convolutional Neural Networks Based on RRAM Devices for Image Recognition and Online Learning Tasks RRAM-based hardware implementation of CNN; expand kernel to the size of image 2021 TCAD SJTU Efficient and Robust RRAM-Based Convolutional Weight Mapping With Shifted and Duplicated Kernel shift and duplicate kernel (SDK) convolutional weight mapping architecture; parallel-window size allocation algorithm; kernel synchronization method 2023 VLSI-SoC Aachen Mapping of CNNs on multi-core RRAM-based CIM architectures architecture optimized for communication; compiler algorithms for conv2D layer; cycle-accurate simulator 2023 TODAES UCAS Mathematical Framework for Optimizing Crossbar Allocation for ReRAM-based CNN Accelerators formulate a crossbar allocation problem for ReRAM-based CNN accelerators; dynamic programming based solver; models the performance considering allocation problem 2025 TVLSI NBU A 578-TOPS/W RRAM-Based Binary Convolutional Neural Network Macro for Tiny AI Edge Devices ReRAM XNOR cell; BCNN CIM macro with FPGA as the control core 4 4 3"},{"location":"hardware/memory/#rram-cim-transformer-accelerator","title":"RRAM CIM: Transformer Accelerator","text":"<p>Challenge: RRAM's cross-bar architecture is suitable for matrix operations.</p> Year Venue Authors Title Tags P E N 2023 VLSI Purdue X-Former: In-Memory Acceleration of Transformers in-memory accelerate attention layers; intralayer sequence blocking dataflow; provides a simulator 2024 TODAES HUST A Cascaded ReRAM-based Crossbar Architecture for Transformer Neural Network Acceleration cascaded crossbar arrays that uses transimpedance amplifiers; data mapping scheme to store signed operands; ADC virtualization scheme 2023 VLSI HUST An RRAM-Based Computing-in-Memory Architecture and Its Application in Accelerating Transformer Inference RRAM-based in-memory floating-point computation architecture (RIME); pipelined implementations of MatMul and softmax 3 3 4 2020 ICCAD Duke ReTransformer: ReRAM-based processing-in-memory architecture for transformer acceleration MatMul does matrix decomposition in scaled dot-product attention; in-memory logic techniques for softmax; sub-matrix pipeline 4 3 3 2022 TCAD KAIST A Framework for Accelerating Transformer-Based Language Model on ReRAM-Based Architecture window self-attention and window-size search algorithm; ReRAM hardware design optimized for this algorithm 4 2 3 2020 ICCD LSU ATT: A Fault-Tolerant ReRAM Accelerator for Attention-based Neural Networks ReRAM-based accelerator with pipeline for AttNNs; heuristic redundancy algorithm 3 2 2"},{"location":"hardware/memory/#rram-cim-special-usage","title":"RRAM CiM: Special Usage","text":"Year Venue Authors Title Tags P E N 2023 GLSVLSI Yale Examining the Role and Limits of Batchnorm Optimization to Mitigate Diverse Hardware-noise in In-memory Computing non-idealities; circuit-level parasitic resistances and device-level non-idealities; crossbar-aware fine-tuning of batchnorm parameters 2019 ASPDAC POSTECH In-memory batch-normalization for resistive memory based binary neural network hardware in-memory batchnormalization schemes; integrate BN layers on crossbar 2024 TRETS UFRGS Reprogrammable Non-Linear Circuits Using ReRAM for NN Accelerators perform typical non-linear operations using ReRAM 4 3 4 2019 Adv. Funct. Mater. HUST Functional Demonstration of a Memristive Arithmetic Logic Unit (MemALU) for In\u2010Memory Computing non-volatile Boolean logic using RRAM crossbar;reconfigurable boolean logic gates 3 4 3"},{"location":"hardware/memory/#cim-hybrid-architecture","title":"CIM: Hybrid Architecture","text":"<p>Solution: Use hybrid architecture (like SRAM + RRAM) to overcome the limitations of single device (e.g. RRAM's non-ideal effects).</p>"},{"location":"hardware/memory/#hybrid-cim-sram-general-logic","title":"Hybrid CIM: SRAM + General Logic","text":"Year Venue Authors Title Tags P E N 2023 GLSVLSI USC Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units hybrid TPU-IMAC architecture; TPU for conv, CIM for fc 2025 ASPLOS CAS PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System dynamic parallelism-aware task scheduling for llm decoding; online kernel characterization for heterogeneous architectures; hybrid PIM units for compute-bound and memory-bound kernels"},{"location":"hardware/memory/#hybrid-cim-sram-rram","title":"Hybrid CIM: SRAM + RRAM","text":"Year Venue Authors Title Tags P E N 2024 Science NTHU Fusion of memristor and digital compute-in-memory processing for energy-efficient edge computing Fusion of ReRAM and SRAM CiM; ReRAM SLC &amp; MLC Hybrid; Current quantization; Weight shifting with compensation 2024 IPDPS Georgia Tech Harmonica: Hybrid Accelerator to Overcome Imperfections of Mixed-signal DNN Accelerators select and transfer imperfectionsensitive weights to digital accelerator; hybrid quantization(weights on analog part is more quantized) 2023 ICCAD SJTU TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations DCpower-free weight-restore from ReRAM; ternary SRAM-CIM mechanism with differential computing scheme"},{"location":"hardware/memory/#hybrid-cim-memristormram-sram","title":"Hybrid CIM: Memristor/MRAM + SRAM","text":"Year Venue Authors Title Tags P E N 2025 Nature TSMC A mixed-precision memristor and SRAM compute-in-memory AI processor layer based INT-FP hybrid architure; kernel-based mix-CIM (SRAM/ReRAM/digital hybrid architecture) 5 5 2 2025 DAC Chung-Ang Univ. HH-PIM: Dynamic Optimization of Power and Performance with Heterogeneous-Hybrid PIM for Edge AI Devices heterogeneous-hybrid PIM with HP/LP modules and MRAM/SRAM; dynamic data placement algorithm for energy optimization; dual PIM controller design 3 4 2"},{"location":"hardware/memory/#hybrid-cim-analog-digital","title":"Hybrid CIM: Analog + Digital","text":"Year Venue Authors Title Tags P E N 2023 arXiv HP RACE-IT: A Reconfigurable Analog CAM-Crossbar Engine for In-Memory Transformer Acceleration Compute Analog Content Addressable Memory (Compute-ACAM) structure; accelerator based on crossbars and Compute-ACAMs; encoding-based optimization 3 3 4 2024 VLSI FDU HARDSEA: Hybrid Analog-ReRAM Clustering and Digital-SRAM In-Memory Computing Accelerator for Dynamic Sparse Self-Attention in Transformer product-quantization-based sparse self-attention algorithm; ADC-free ReRAM-CIM macro; ReRAM-CIM for front-end attention sparsification, SRAM-CIM for back-end sparse attention 4 3 3 2024 ASP-DAC Keio OSA-HCIM: On-The-Fly Saliency-Aware Hybrid SRAM CIM with Dynamic Precision Configuration On-the-fly Saliency-Aware precision configuration scheme; Hybrid CIM Array for DCIM and ACIM using split-port SRAM 2025 arXiv South Carolina PIM-LLM: A High-Throughput Hybrid PIM Architecture for 1-bit LLMs hybrid PIM-Digital architecture; analog PIM for low-precision MatMul; digital systolic array for high-precision matMul 4 3 1 2024 ESSERC UCSD An Analog and Digital Hybrid Attention Accelerator for Transformers with Charge-based In-memory Computing analog CIM for low-score tokens, digital processor for high 3 4 2"},{"location":"hardware/memory/#cim-quantization","title":"CIM: Quantization","text":"<p>Challenge: Limited by the precision &amp; area &amp; power trade-off of the ADC; certain CIM devices like RRAM are not suitable for high-precision computation (e.g. FP32). Quantization is needed to reduce the precision of the data.</p>"},{"location":"hardware/memory/#cim-quantization-for-analog-cim","title":"CIM Quantization: For Analog CIM","text":"Year Venue Authors Title Tags P E N 2023 ISLPED Purdue Partial-Sum Quantization for Near ADC-Less Compute-In-Memory Accelerators ADC-Less and near ADC-Less CiM accelerators; CiM hardware aware DNN quantization methodology 2023 AICAS TU Delft Mapping-aware Biased Training for Accurate Memristor-based Neural Networks favorability constraint analysis to find important weight values; mapping-aware biased training to restrict weight values to low variance RRAM states 3 4 2 2024 TCAD BUAA CIMQ: A Hardware-Efficient Quantization Framework for Computing-In-Memory-Based Neural Network Accelerators bit-level sparsity induced activation quantization; quantizing partial sums to decrease required resolution of ADCs; arraywise quantization granularity 2024 TCAD BUAA CIM\u00b2PQ: An Arraywise and Hardware-Friendly Mixed Precision Quantization Method for Analog Computing-In-Memory mixed precision quantization method based on evolutionary algorithm; arraywise quantization granularity; evaluation method to obtain the performance of strategy on the CIM 2024 ICCAD TU Delft Hardware-Aware Quantization for Accurate Memristor-Based Neural Networks analysis of fixed-point quantization impact on conductance variation; weight quantization tuning technique; approach to reduce the residual error 3 2 3"},{"location":"hardware/memory/#cim-quantization-for-all-cim","title":"CIM Quantization: For all CIM","text":"Year Venue Authors Title Tags P E N 2018 CVPR Google Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference integer-only inference arithmetic; quantizes both weights and activations as 8-bit integers, bias 32-bit; provides both quantized inference framework and training frame work 2023 ICCD SJTU PSQ: An Automatic Search Framework for Data-Free Quantization on PIM-based Architecture post-training quantization framework without retraining; hardware-aware block reassembly"},{"location":"hardware/memory/#cim-digital-cim","title":"CIM: Digital CIM","text":"Year Venue Authors Title Tags P E N 2025 ISCAS CAS StreamDCIM: A Tile-based Streaming Digital CIM Accelerator with Mixed-stationary Cross-forwarding Dataflow for Multimodal Transformer tile-based reconfigurable CIM macro microarchitecture; mixed-stationary cross-forwarding dataflow; ping-pong-like finegrained compute-rewriting pipeline"},{"location":"hardware/memory/#nvm","title":"NVM","text":"Year Venue Authors Title Tags P E N 2024 ISCAS UMCP On-Chip Adaptation for Reducing Mismatch in Analog Non-Volatile Device Based Neural Networks float-gate transistors based; hot-electron injection to address the issue of mismatch and variation 2023 DATE UniBo End-to-End DNN Inference on a Massively Parallel Analog In Memory Computing Architecture many-core heterogeneous architecture; general-purpose system based on RISC-V cores and nvAIMC cores; based on Phase-Change Memory(PCM);"},{"location":"hardware/network/","title":"Interconnection Networks","text":""},{"location":"hardware/network/#network-on-chip","title":"Network-on-Chip","text":"<p>Challenge: The bandwidth limitations and low communication efficiency faced by traditional bus architectures in on-chip multi-core &amp; many-core systems.</p> <p>Solution: NoC provides a flexible and high-bandwidth communication infrastructure for heterogeneous chiplets; enables efficient data movement and processing in heterogeneous many-core architectures.</p>"},{"location":"hardware/network/#wafer-scale","title":"Wafer-Scale","text":"Year Venue Authors Title Tags P E N 2024 SC THU Switch-Less Dragonfly on Wafers: A Scalable Interconnection Architecture based on Wafer-Scale Integration four-level topology structure; minimal routing algorithm on dragonfly for VC vumber reduction 2024 TCAS SYSU CINOC: Computing in Network-On-Chip With Tiled Many-Core Architectures for Large-Scale General Matrix Multiplications computable input buffers;  thread execution free from fine-grained instruction control; data-aware thread execution 2025 ISCA THU PD Constraint-aware Physical/Logical Topology Co-Design for Network on Wafer mesh-switch physical topology; dual-granularity logical topology 4 3 2"},{"location":"hardware/network/#topology","title":"Topology","text":"<p>Challenge: Current NoC topologies are often rigid and not adaptable to the specific needs of heterogeneous chiplets.</p> Year Venue Authors Title Tags P E N 2021 HPCA GWU Adapt-NoC: A Flexible Network-on-Chip Design for Heterogeneous Manycore Architectures mux based adaptable router architecture; adaptable link design; reinforcement learning based subNoC optimization algorithm 2022 HPCA Huawei Application Defined On-chip Networks for Heterogeneous Chiplets: An Implementation Perspective bufferless multi-ring NoC design; application-architecture-physical co-design method; architecture expressiveness; deadlock resolution SWAP mechanism 2024 MICRO THU Ring Road: A Scalable Polar-Coordinate-based 2D Network-on-Chip Architecture Ring Road topology based on isolated cycles and trees; polar coordinate DOR(dimension-order-routing); inter/intra-chip decouple routing algorithm 2024 arXiv WSU Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures heterogeneous 3D NoC; pipeline design across heterogeneous resources; crossbar-wise quantization 2024 ISLPED WSU HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for Transformer Acceleration 3D integration;  distinct planar tiers where each tier is tailor-made for either MHA or the FF network; alleviate memory bottlenecks while preventing frequent rewrites on ReRAM crossbars"},{"location":"hardware/network/#interconnect","title":"Interconnect","text":"Year Venue Authors Title Tags P E N 2012 SIGCOMM CMU On-Chip Networks from a Networking Perspective: Congestion and Scalability in Many-Core Interconnects congestion control mechanism for bufferless NoC; interval-based congestion control algorithm; simple injection throttling algorithm 2023 ICCAD UCF ARIES: Accelerating Distributed Training in Chiplet-based Systems via Flexible Interconnects directional bypassing link; ARIES link with transistor; ARIES all-reduce optimization algorithm 2023 MICRO THU Heterogeneous Die-to-Die Interfaces: Enabling More Flexible Chiplet Interconnection Systems heterogeneous interface hetero-PHY and hetero-channel; hetero-channel routing algorithm; application-aware scheduling"},{"location":"hardware/network/#processing-on-noc","title":"Processing on NoC","text":"<p>Challenge: The need for efficient data processing and computation on-chip to reduce data movement and improve performance.</p> Year Venue Authors Title Tags P E N 2017 ISVLSI Ruhr-Universit\u00e4t Bochum Data Stream Processing in Network-on-Chip data stream processing unit(DSPU); operation mode based DSPU programming framework 2019 HPCA TAMU Active-Routing: Compute on the Way for Near-Data Processing active-routing tree; vector processing in cache block for regular access pattern; data prefetch for irregular access pattern 2020 HPCA Drexel University SnackNoC: Processing in the Communication Layer communication fabric quantification; central packet manager for instruction flit; router compute unit as dataflow pe"},{"location":"hardware/network/#traffic-control","title":"Traffic Control","text":"<p>Challenge: The need for efficient traffic control to manage network traffic and reduce congestion and power consumption.</p> Year Venue Authors Title Tags P E N 2017 ISCA TAMU APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures value approximate technique VAXX; encoder/decoder module pair for data compression; approximate value compute logic 2017 ICCD HIT ABDTR: Approximation\u2013Based Dynamic Traffic Regulation for Networks\u2013on\u2013Chip Systems approximate computing based dynamic traffic regulation technique; lightweight design including controller, throttler and approximater 2019 DATE SCUT ACDC: An Accuracy- and Congestion-aware Dynamic Traffic Control Method for Networks-on-Chip quality loss and network congestion modeling; autoregressive model based flow prediction method 2025 arXiv NTU Learning Cache Coherence Traffic for NoC Routing Design cache coherence traffic analyzer; DRL based topology selection and routing design 2 3 2"},{"location":"hardware/network/#fault-tolerant-communication","title":"Fault-Tolerant Communication","text":"Year Venue Authors Title Tags P E N 2014 VLSI ICT ZoneDefense: A Fault-Tolerant Routing for 2-D Meshes Without Virtual Channels fault chains based faulty blocks construction; floor/ceiling rule based defense zone forming; L/F chain routing 2017 TPDS NTU Path-Diversity-Aware Fault-Tolerant Routing Algorithm for Network-on-Chip Systems path diversity analysis; fault-location-based path diversity; PDA-FTR algorithm 2019 DATE UMich SiPterposer: A Fault-Tolerant Substrate for Flexible System-in-Package Design blowing based customized topology; lightweight ECC module based defect tolerance 2022 DATE Colorado State University DeFT: A Deadlock-Free and Fault-Tolerant Routing Algorithm for 2.5D Chiplet Networks virtual network based deadlock freedom; congestion-aware vertical link selection"},{"location":"hardware/network/#router","title":"Router","text":"Year Venue Authors Title Tags P E N 2016 HPCA KTH DVFS for NoCs in CMPs: A Thread Voting Approach thread voting based DVFS machenism; pre-defined region-based V/F adjustment algorithm 2022 HPCA Chalmers FastTrackNoC: A NoC with FastTrack Router Datapaths non-turning hops; direct FastTrack flit path; zero-load latency analysis 2022 HPCA UToronto Stay in your Lane: A NoC with Low-overhead Multi-packet Bypassing FastFlow flow controll method; time-division-multiplexed (TDM) based non-overlapping FastPass-lanes; FastPass for throughput enhancement 2023 HPCA THU A Scalable Methodology for Designing Efficient Interconnection Network of Chiplets interface grouping; hypercube construction algorithm; deadlock-free adaptive routing algorithm; safe/unsafe flow control; network interleaving method 2025 arXiv SJTU StreamGrid: Streaming Point Cloud Analytics via Compulsory Splitting and Deterministic Termination compulsory splitting for reducing on-chip buffer size; deterministic termination for regularizing non-deterministic operations; line buffer optimization for point cloud pipelines; ILP-based buffer size minimization"},{"location":"hardware/network/#remote-procedure-call","title":"Remote Procedure Call","text":"Year Venue Authors Title Tags P E N 2023 arXiv ICT CXL over Ethernet: A Novel FPGA-based Memory Disaggregation Design in Data Centers combining CXL and Ethernet for low-latency remote memory access; FPGA-based prototype with cache optimization; switch-independent congestion control algorithm; native memory semantics for transparent access 2024 arXiv UCSD Telepathic Datacenters: Fast RPCs using Shared CXL Memory pointer-passing RPC over CXL; MPK-based sandboxing for RPC safety; Seal/Release mechanism for RPC safety; RDMA fallback for RPC scalability; Lease/Quota shared memory management 4 3 3"},{"location":"hardware/network/#rdma","title":"RDMA","text":"Year Venue Authors Title Tags P E N 2024 arXiv UCR GPUVM: GPU-driven Unified Virtual Memory GPUVM architecture for on-demand paging; RDMA-capable NIC for GPU memory management; GPU thread-based memory management and page migration; reuse-oriented paged memory for efficient eviction; high-level programming abstraction for GPU memory extension"},{"location":"hardware/parallel/","title":"Parallel and Multi-Processor Architecture","text":""},{"location":"hardware/parallel/#heterogeneous-architecture","title":"Heterogeneous Architecture","text":"<p>Challenge: Classic Heterogeneous Architecture faces challenges in the data movement and memory access patterns; leading to performance bottlenecks.</p> Year Venue Authors Title Tags P E N 2017 TACO Intel HAShCache: Heterogeneity-Aware Shared DRAMCache for Integrated Heterogeneous Systems heterogeneity-aware DRAMCache scheduling PrIS; temporal bypass ByE; spatial occupancy control chaining 2018 ICS NC State ProfDP: A Lightweight Profiler to Guide Data Placement in Heterogeneous Memory Systems latency sensitivity; bandwidth sensitivity; moving factor based data placement 2023 HPCA THU Baryon: Efficient Hybrid Memory Management with Compression and Sub-Blocking stage area and selective commit for stable block; dual-format metadata scheme; cacheline-aligned compression and two-level replacements"},{"location":"hardware/parallel/#cpu-gpu-system","title":"CPU-GPU System","text":"Year Venue Authors Title Tags P E N 2024 arXiv KTH Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper Grace Hopper system memory characterization; integrated CPU-GPU page table analysis; first-touch policy impact study; system page size impact study; access-counter page migration evaluation 2 4 3"},{"location":"hardware/parallel/#disaggregated-memory","title":"Disaggregated Memory","text":"<p>Challenge: CXL and NVM offer higher speed &amp; bandwidth than storage devices with byte-level access. Memory disaggregation using DRAM (high-speed/BW + small capacity) and NVM (low-speed/BW + large capacity), faces latency, bandwidth, and consistency challenges.</p>"},{"location":"hardware/parallel/#cxl-based-disaggregated-memory","title":"CXL-based Disaggregated Memory","text":"Year Venue Authors Title Tags P E N 2025 ASPLOS Yale PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated Memory iterator-based programming model; disaggregated accelerator architecture; in-network routing for distributed traversal 3 4 3 2025 ASPLOS Purdue EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation Ethernet PHY network stack; PHY in-network scheduler; PHY intra-frame preemption 4 4 4 2025 arXiv Micron Architectural and System Implications of CXL-enabled Tiered Memory CXL parallelism bottleneck analysis; Unfair queuing analysis; MIKU dynamic request control; ToR-based service time estimation; Hierarchical CXL throttling 4 4 3"},{"location":"hardware/parallel/#survey","title":"Survey","text":"Year Venue Authors Title Tags P E N 2025 arXiv SJTU Survey of Disaggregated Memory: Cross-layer Technique Insights for Next-Generation Datacenters Cross-layer classification of DM techniques; hardware-level categories; architectural-level classifications; system and runtime-level groupings; application-level optimizations such as general-purpose and domain-specific approaches"},{"location":"hardware/parallel/#chiplets","title":"Chiplets","text":"<p>Challenge: Current chip designs are often monolithic and inflexible; leading to high costs and limited performance optimization opportunities.</p> <p>Solution: Use chiplets to enable more flexible and cost-effective system designs by allowing the integration of specialized dies manufactured using optimal processes; leading to improved performance and yield.</p>"},{"location":"hardware/parallel/#survey_1","title":"Survey","text":"Year Venue Authors Title Tags P E N 2020 Electronics NUDT Chiplet Heterogeneous Integration Technology\u2014Status and Challenges heterogeneous integration technology; interconnect interfaces and protocols; packaging technology 2022 CCF THPC ICT Survey on chiplets: interface, interconnect and integration methodology development history; interfaces and protocols; packaging technology; EDA tool; standardization of chiplet technology 2024 IEEE CASS THU Chiplet Heterogeneous Integration Technology\u2014Status and Challenges wafer-scale chip architecture; compiler tool chain; integration technology; wafer-scale system; fault tolerance"},{"location":"hardware/parallel/#cost-analysis","title":"Cost Analysis","text":"Year Venue Authors Title Tags P E N 2025 arXiv ASU CATCH: a Cost Analysis Tool for Co-optimization of chiplet-based Heterogeneous systems heterogeneous chiplet system modeling; DSE on chiplets size,IO,connection"},{"location":"hardware/parallel/#3d-ic","title":"3D IC","text":"<p>Solution: 3DIC technology enables higher integration density; shorter interconnects; and improved performance by stacking multiple active layers in a single device.</p>"},{"location":"hardware/parallel/#general-3d-ic","title":"General 3D IC","text":"Year Venue Authors Title Tags P E N 2019 GLSVLSI Boston Univeristy An Overview of Thermal Challenges and Opportunities for Monolithic 3D ICs TSV-based 3D integration; Mono3D integration with nanoscale monolithic inter-tier vias; influence of lateral heat flow and inter-connection 2019 ECTC TSMC System on Integrated Chips (SoIC) for 3D Heterogeneous Integration system on integrated chips; SoIC package integration; reliability of SoIC bond,TSV and TDV 2020 DATE Georgia Tech Macro-3D: A Physical Design Methodology for Face-to-Face-Stacked Heterogeneous 3D ICs face-to-face stack; separate 2D floorplans generation; memory-on-logic projection 2022 IEEE Micro Cerebras Cerebras Architecture Deep Dive: First Look Inside the Hardware/Software Co-Design for Deep Learning fine-grained dataflow scheduling; high-bandwidth, low-latency fabric design; weight streaming"},{"location":"hardware/parallel/#interconnection","title":"Interconnection","text":"Year Venue Authors Title Tags P E N 2025 HPCA Fudan EIGEN: Enabling Efficient 3DIC Interconnect with Heterogeneous Dual-Layer Network-on-Active-Interposer Dual-layer interconnect architecture, Reinforcement learning routing, Switch-programmable interconnect 3 2 3"},{"location":"hardware/parallel/#design-space-exploration","title":"Design Space Exploration","text":"Year Venue Authors Title Tags P E N 2025 arXiv SJTU Cool-3D: An End-to-End Thermal-Aware Framework for Early-Phase Design Space Exploration of Microfluidic-Cooled 3DICs end-to-end thermal-aware framework; microfluidic cooling integration; Pre-RTL design space exploration; floorplan designer; microfluidic cooling strategy generator"},{"location":"hardware/parallel/#benchmarks","title":"Benchmarks","text":"Year Venue Authors Title Tags P E N 2025 arXiv NJU Open3DBench: Open-Source Benchmark for 3D-IC Backend Implementation and PPA Evaluation open-source 3D-IC benchmark; modular 3D partitioning and placement; Open3D-DMP algorithm for cross-die co-placement; comprehensive PPA evaluation with thermal simulation"},{"location":"hardware/perf/","title":"Performance Modeling and Analysis","text":""},{"location":"hardware/perf/#hardware-performance-counter","title":"Hardware Performance Counter","text":"<p>Challenge: Software performance analysis and optimization is often limited by the lack of accurate and detailed information about the underlying hardware behavior.</p> <p>Solution: Use hardware performance counters to gather data on CPU usage; memory access patterns; cache hits/misses; branch predictions; and other metrics that can help analyze the performance of software applications and hardware systems.</p>"},{"location":"hardware/perf/#survey","title":"Survey","text":"Year Venue Authors Title Tags P E N 2013 TODAES Crete A Survey and Taxonomy of On-Chip Monitoring of Multicore Systems-on-Chip debugging/performance/QoS monitor; physical parameter monitor; methodology based taxonomy 2 4 1 2016 CSUR Oak Ridge Lab Understanding GPU Power: A Survey of Profiling, Modeling, and Simulation Methods external/internal power measurement; HPC based power model; GPU power simulation 3 3 1 2019 SP UNC-Chapel Hill SoK: The Challenges, Pitfalls, and Perils of Using Hardware Performance Counters for Security non-determinism and overcounting effects; performance monitoring interrupt 3 4 1"},{"location":"hardware/perf/#specific-application","title":"Specific Application","text":"Year Venue Authors Title Tags P E N 2000 SC UT A Scalable Cross-Platform Infrastructure for Application Performance Tuning Using Hardware Counters portable and machine-dependent layers based architecture; eventset for group management; counter multiplexing 2 4 2 2004 SC UMD Using Hardware Counters to Automatically Improve Memory Performance two-phase dynamic page migration algorithm; sun fire link counter 3 4 3 2013 ISPASS UTAustin Non-determinism and overcount on modern hardware performance counter implementations nondeterministic hardware interrupts; float point unit related overcount; retired instruction overcount 2 4 2 2020 CONECCT IIIT Power, Performance And Thermal Management Using Hardware Performance Counters fine-grained dynamic voltage and frequency scaling; PMC-based power and temperature correlation model; thermal zone and partition-based management 2 4 2"},{"location":"hardware/perf/#architecture-design","title":"Architecture Design","text":"<p>Challenge: Existing hardware performance counters provide limited information; expansion is needed to support more hardware behavior data.</p> Year Venue Authors Title Tags P E N 2006 ASPLOS UW\u2013Madison A Performance Counter Architecture for Computing Accurate CPI Components interval analysis based performance model; frontend miss table(FMT); shared FMT 3 3 2 2014 ISPASS Intel A Top-Down Method for Performance Analysis and Counters Architecture top-down bottleneck analysis method; frontend bound; bad speculation; retiring; backend bound; top-down performance events 3 5 3 2015 ISCA ANU Computer Performance Microscopy with SHIM double-time error correction; sample periods randomizing; CMP core sampling for low overhead 4 4 3"},{"location":"hardware/perf/#dataflow-architecture","title":"Dataflow Architecture","text":"Year Venue Authors Title Tags P E N 2022 OSDI UCB Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning inter-operator parallelisms; intra-operator parallelisms; ILP and DP hierarchical optimization 2023 MICRO PKU TileFlow: A Framework for Modeling Fusion Dataflow via Tree-based Analysis 3D design space of fusion dataflow; tree-based description; tile-centric notation 2024 ISCA Stanford The Dataflow Abstract Machine Simulator Framework communicating sequential processes; event-queue free execution; context-channel based description; asynchronous distributed time"},{"location":"hardware/perf/#connection-architecture","title":"Connection Architecture","text":"Year Venue Authors Title Tags P E N 2014 JPDC Inria Versatile, scalable, and accurate simulation of distributed applications and platforms API based communication&amp;computation description; informed model of TCP for moderate size grids; file based modular network representation technique 2020 MICRO Georgia Tech; NVIDIA MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of DNN Mappings data-centric mapping; data reuse analysis; TemperalMap; SpatialMap; analytical cost model 2023 ISPASS Georgia Tech ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale graph-based training-loop execution; multi-dimensional heterogeneous topology construction; analytical network backend 2024 ATC THU Evaluating Chiplet-based Large-Scale Interconnection Networks via Cycle-Accurate Packet-Parallel Simulation packet-centric simulation; critical resources recorading for process-order-induced deviations; unimportant stages elimination 2025 arXiv UCLM Understanding Intra-Node Communication in HPC Systems and Datacenters Intra-/inter-node communication interference; Packet-level simulation (OMNeT++); PCIe/NVLink modeling; LLM communication patterns (DP, TP, PP) impact"},{"location":"hardware/perf/#redundancy-detection","title":"Redundancy Detection","text":"<p>Challenge: Redundant zeros in data can lead to inefficiencies in software performance; making it important to detect and eliminate them.</p> Year Venue Authors Title Tags P E N 2020 SC NC State ZeroSpy: Exploring Software Inefficiency with Redundant Zeros code-centric analysis for instruction detection; data-centric analysis for data detection 2020 SC NC State GVPROF: A Value Profiler for GPU-Based Clusters temporal/spatial load/store redundancy; hierarchical sampling for reducing monitoring overhead; bidirectional search algorithm on dependency graph 2022 ASPLOS NC State ValueExpert: Exploring Value Patterns in GPU-accelerated Applications value-related inefficiencies data value pattern recoginition; value flow graph; parallel intervals merging algorithm 2022 SC NC State Graph Neural Networks Based Memory Inefficiency Detection Using Selective Sampling dead store; silent store; silent load; assembly-level procedural control-flow embedding; dynamic value semantic embedding; relative positional encoding for different compilation options"},{"location":"hardware/perf/#variation-impact","title":"Variation Impact","text":"<p>Solution: Characterize sources of variation (hardware; software; environment); develop models to predict variation impact; implement techniques to reduce variation (e.g., dynamic voltage and frequency scaling, adaptive scheduling).</p> Year Venue Authors Title Tags P E N 2009 HPCMP UCSD Measuring and Understanding Variation in Benchmark Performance MPI communication variation; distribution of performance variation 2016 SC UNM Understanding Performance Interference in Next-Generation HPC Systems extreme value theory; bulk-synchronous parallel based modeling; gang/earliest deadline first scheduling"},{"location":"hardware/perf/#stall-attribution","title":"Stall Attribution","text":"<p>Challenge: Stall can be caused by hardware or software; identifying the root cause of stalls and their impact on performance is crucial for performance optimization.</p> Year Venue Authors Title Tags P E N 2023 ICPE NC State University DrGPU: A Top-Down Profiler for GPU device memory stall; synchronization stall; instruction related stall; shared memory related stall 2024 MICRO NUDT HyFiSS: A Hybrid Fidelity Stall-Aware Simulator for GPGPUs memory/compute structual/data stall; synchronization stall; control stall; idle stall; cooperative thread array-sets based SM sampling algorithm"},{"location":"hardware/processor/","title":"Processor Architecture","text":""},{"location":"hardware/processor/#cache","title":"Cache","text":"<p>Challenge: Managing shared cache resources (e.g. LLC) efficiently in multi-core/multi-programmed environments.</p> Year Venue Authors Title Tags P E N 2006 MICRO Intel Molecular Caches: A caching structure for dynamic creation of application-specific Heterogeneous cache regions molecular cache architecture; application space identifier based cache partition; randy replacement algorithm 2011 ISCA Stanford Vantage: Scalable and Efficient Fine-Grain Cache Partitioning managed-unmanaged region division; churn-based management; feedback-based aperture control 2016 HPCA Intel Cache QoS: From concept to reality in the Intel\u00ae Xeon\u00ae processor E5-2600 v3 product family cache monitoring technology; cache allocation technology; resource monitoring IDs (RMIDs); classes of service (CLOS) 2018 EuroSys PKU DCAPS: dynamic cache allocation with partial sharing dynamic fine-grained shared cache management; balance cache utilization and contention; online practical miss rate curve"},{"location":"hardware/processor/#multi-level-cache","title":"Multi-Level Cache","text":"<p>Challenge: Optimizing the interaction and data movement between different levels of the cache hierarchy (e.g. L1 - L2 - L3) in Chip Multi-Processors is complex.</p> Year Venue Authors Title Tags P E N 2005 ISCA IBM Adaptive mechanisms and policies for managing cache hierarchies in chip multiprocessors limit unnecessary clean write backs; write back L2 to peer L2; second adaptive mechanism 2012 JIP Tokio Tech Autonomous L3 Cache Technolgy for High Responsiveness autonomous L3 cache; trio-configuration architecture; autonomous decentralized multi-layer cache"},{"location":"hardware/processor/#vector-unit","title":"Vector Unit","text":""},{"location":"hardware/processor/#memory-access","title":"Memory Access","text":"<p>Challenge: Vector memory access is not well supported &amp; optimized in the current microarchitecture like RISC-V.</p> Year Venue Authors Title Tags P E N 2025 CF Companion ETHZ AraOS: Analyzing the Impact of Virtual Memory Management on Vector Unit Performance virtual memory management for RVV; performance analysis of virtual memory overhead 2 2 1 2025 arXiv THU Efficient Architecture for RISC-V Vector Memory Access data reorganization module; load/store data organization; row/column-accessible vector register file 2 4 3"},{"location":"software/","title":"Software","text":"<p>The reviewed software papers are classified into the following groups:</p>"},{"location":"software/#algorithms-and-theory","title":"Algorithms and Theory","text":"<p>Focuses on the foundations of computation and problem-solving. Includes:</p> <ul> <li>Algorithm design and analysis (including ML/DL algorithms)</li> <li>Data structures</li> <li>Computational complexity</li> <li>Computability theory</li> </ul>"},{"location":"software/#programming-languages-and-compilers","title":"Programming Languages and Compilers","text":"<p>Concerns the design, implementation, and analysis of programming languages. Includes:</p> <ul> <li>Language design and semantics</li> <li>Compiler construction and optimization</li> <li>Program analysis</li> </ul>"},{"location":"software/#operating-systems","title":"Operating Systems","text":"<p>Focuses on managing computer hardware and providing system services. Includes:</p> <ul> <li>Kernel design and implementation</li> <li>Process and thread management</li> <li>Memory management</li> <li>File systems and storage</li> <li>Virtualization</li> <li>Synchronization</li> </ul>"},{"location":"software/#distributed-systems-and-networking","title":"Distributed Systems and Networking","text":"<p>Covers systems of multiple interacting components and communication. Includes:</p> <ul> <li>Distributed algorithms (e.g., consensus, replication)</li> <li>Cloud computing platforms and architectures</li> <li>Microservices</li> </ul>"},{"location":"software/#high-performance-computing-hpc","title":"High-Performance Computing (HPC)","text":"<ul> <li>Parallel programming models (e.g., MPI, OpenMP, CUDA)</li> <li>Scientific computing applications and libraries</li> <li>Parallel algorithms</li> </ul>"},{"location":"software/#performance-analysis","title":"Performance Analysis","text":"<ul> <li>Profilers (sampling, instrumentation)</li> <li>Simulators and emulators (for software/system analysis)</li> <li>Benchmarking methodologies and suites</li> <li>Workload characterization</li> </ul>"},{"location":"software/algorithm/","title":"Algorithms, Theory, and Formal Methods","text":""},{"location":"software/algorithm/#algorithm-design-and-analysis","title":"Algorithm design and analysis","text":"<p>Solution: an algorithm is a well-defined, finite sequence of steps that solves a specific problem or accomplishes a particular task. We focus on algorithms that can solving problems.</p>"},{"location":"software/algorithm/#ml-algorithms","title":"ML Algorithms","text":"<p>Soultion: ML algorithms are fundamental tools that enable computers to learn from data and make predictions or decisions without being explicitly programmed.</p>"},{"location":"software/algorithm/#llm-algorithm","title":"LLM Algorithm","text":"<p>Solution: enable ai chat with human, some people think is the way to AGI.</p> Year Venue Authors Title Tags P E N 2020 arXiv OpenAI Scaling Laws for Neural Language Models fundamentals of LLM; increase model size and performance raise 4 5 5"},{"location":"software/algorithm/#llm-transformer","title":"LLM Transformer","text":"<p>Solution: Transformer is an old algorithm, which have many problems like square complexity. These problems raise new algorithms to fix the old architecture.</p> Year Venue Authors Title Tags P E N 2019 arXiv Google Fast Transformer Decoding: One Write-Head is All You Need MQA; share same KV cache for all heads; multi-query attention 1 4 3 2024 NeuroComputing ZhuiYi RoFormer: Enhanced Transformer with Rotary Position Embedding use rotary position embedding to fix the problem of long context; nter-word dependencies decay gradually with the increase of relative distance 3 4 3 2025 arXiv Qwen Parallel Scaling Law for Language Models enhance model's parallel ability to enhance the performance instead of increasing the model size; parallel multi output and conclude one output 4 4 4"},{"location":"software/algorithm/#llm-alignment","title":"LLM Alignment","text":"<p>Solution: LLM alignment aims to make LLM outputs more consistent with user intent. Its challenges are ensuring safety, addressing multi-modal complexities, and balancing inference ability with alignment.</p> Year Venue Authors Title Tags P E N 2024 arXiv SJTU Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation social scene simulation; emulate realistic multiparty interactions and consequences; monopolylogue 2025 ICLR Princeton Safety Alignment Should Be Made More Than Just a Few Tokens Deep ai-savety centered alignment; enhance sacety on deeper tokens and data 3 3 3"},{"location":"software/algorithm/#llm-finetune","title":"LLM Finetune","text":"<p>Solution: finetune adapts a pre-trained model to a specific task or domain. By doing so, the model can better fit the specific task or domain.</p> Year Venue Authors Title Tags P E N 2021 ICLR Miscrosoft LoRA: Low-Rank Adaptation of Large Language Models split the weight matrix into two parts; reduce the number of parameters to finetune 2 4 4"},{"location":"software/algorithm/#coding-llm-finetune","title":"Coding LLM Finetune","text":"Year Venue Authors Title Tags P E N 2024 arXiv UMD HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages large synthetic parallel programming dataset; parallel code generation; HPC AI developer tools"},{"location":"software/algorithm/#llm-powered-ai-agent","title":"LLM-Powered AI Agent","text":"Year Venue Authors Title Tags P E N 2024 arXiv THU LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination hierarchical language agent; real-time human-AI coordination; slow mind &amp; fast mind"},{"location":"software/algorithm/#rl-algorithms","title":"RL Algorithms","text":"<p>Solution: RL learns from rewards or penalties received without labeled data. It takes actions that interact with the environment. It can learn optimal policies in super large config space.</p> Year Venue Authors Title Tags P E N 2015 Nature DeepMind Human-level control through deep reinforcement learning deep reinforcement learning; human-level control; playing Atari games 5 5 3"},{"location":"software/algorithm/#quantization","title":"Quantization","text":"<p>Solution: Quantization are focusing on tradeoffs of accuracy and computation/memory. The challenges are how to run models in high performance and low memory/computation cost.</p>"},{"location":"software/algorithm/#adaptive-datatype","title":"Adaptive Datatype","text":"<p>Solution: Adaptive datatypes aim to optimize numerical representation by dynamically adjusting to the precision and range requirements of data. The challenge lies in balancing computational efficiency, memory usage, and accuracy across diverse tasks and hardware constraints.</p>"},{"location":"software/algorithm/#for-llm","title":"For LLM","text":"Year Venue Authors Title Tags P E N 2025 arXiv Rice 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float dynamic-length float; preserving bit-for-bit identical outputs; BFloat16 exponents carry significantly less information than their allocated bit width 4 4 4 2025 HPCA SJTU M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type group-wise quantization for both weight and KV cache; new encoding paradigm to improve information utilization in group-wise quantization; specific processing element for encoding paradigm 4 4 2 2025 HPCA Cornell BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration introduce additional asymmetry to FP by repurposing a redundant zero value with another special value; hardware accelerator design 3 3 3"},{"location":"software/algorithm/#for-non-llm","title":"For Non-LLM","text":"Year Venue Authors Title Tags P E N 2020 CVPR ByteDance Inc. AdaBits: Neural Network Quantization With Adaptive Bit-Widths joint-quantization method applied in training;Switchable Clipping Level (SCL) between layers 4 3 3 2022 ICLR Snap Inc. F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization variance-based fixed-point format selection for weights and activations; training algorithm for fixed-point models 3 3 2 2022 MICRO SJTU ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization fixed-length adaptive numerical data type; combines the advantages of float and int for adapting to the importance of different values within a tensor; adaptive framework that selects the best type for each tensor 2024 TCAD HKU DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference adaptive data representation with variablelength encoding; hardware-aware quantization framework 2024 arXiv Harvard Nanoscaling Floating-Point (NxFP): NanoMantissa, Adaptive Microexponents, and Code Recycling for Direct-Cast Compression of Large Language Models Nanoscaling Floating-Point (NxFP); NanoMantissa; Adaptive Microexponents; Code Recycling"},{"location":"software/algorithm/#general-method","title":"General method","text":"<p>Solution: General quantization methods aim to optimize the trade-off between model accuracy and computational efficiency. Challenges include addressing layer-specific quantization errors, enhancing fault tolerance, and finding optimal bit-width configurations.</p>"},{"location":"software/algorithm/#for-general-llm","title":"For General LLM","text":"Year Venue Authors Title Tags P E N 2023 ISCA SJTU OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization outlier-victim pair that sacrifices the colocated normal values to accommodate the outliers;OVP-based quantization framework and architectural implementation 4 4 2 2023 ICML MIT SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models offline migrates the quantization difficulty from activations to weights 4 5 3 2024 ISCA SNU Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization \u201cpower of 2\u201d channel decomposition rule; Tender accelerator design 4 3 2 2025 arXiv PKU Bitnet.cpp: Efficient Edge Inference for Ternary LLMs ternary mpGEMM library; avoid intricate bit-level manipulations; achieving lossless inference for BitNet b1.58 2025 AAAI ByteDance ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models block-wise distribution correction and compensation scheme; bit balance strategy 4 3 2"},{"location":"software/algorithm/#kv-cache-specialized","title":"KV Cache specialized","text":"Year Venue Authors Title Tags P E N 2025 arXiv UVa HACK: Homomorphic Acceleration via Compression of the Key-Value Cache for Disaggregated LLM Inference method without dequantization; homomorphic quantization method for matrix multiplication; requantization elimination 2 2 3 2025 arXiv SJTU MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV Product Quantization a non-uniform quantization algorithm based on product quantization; leverages sparse computation and asynchronous quantization; distributes quantization power unevenly across channels 3 4 2"},{"location":"software/algorithm/#for-non-llm_1","title":"For Non-LLM","text":"Year Venue Authors Title Tags P E N 2018 AAAI SUTD Adaptive Quantization for Deep Neural Network measurement to estimate the effect of parameter quantization errors in individual layers;optimization process for finding optimal quantization bit-width for each layer 3 3 4 2020 ISCA SJTU DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration dynamic region-based quantization algorithm; sub-feature map quantization; accelerator architecture for proposing dynamic region-based quantization 4 3 2 2021 MLSys Nvidia VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference per-vector(\u224816-64 elements) scaled quantization technique; two-level scaling scheme and algorithm; modified MAC unit in accelerator 4 3 5 2021 ICML Intel Accurate Post Training Quantization With Small Calibration Sets layer-by-layer optimization method; integer programming; para-normalization 3 3 3 2023 ACML KOBE-U A Mixed-Precision Quantization Method without Accuracy Degradation Using Semilayers semilayers based on whether loss difference is positive or negative 3 2 2"},{"location":"software/algorithm/#fault-tolerance","title":"Fault Tolerance","text":"<p>Solution: Fault tolerance in quantization ensures that models remain robust and reliable despite errors or noise</p> Year Venue Authors Title Tags P E N 2019 DFT Xilinx Efficient Error-Tolerant Quantized Neural Network Accelerators selective channel replication; fault-aware scheduling of processing elements for folded implementations 3 2 3 2023 DAC Yonsei RQ-DNN: Reliable Quantization for Fault-tolerant Deep Neural Networks quantization to enhance fault tolerance caused by fault in memory; quantize to bimodal 3 3 3"},{"location":"software/algorithm/#quantization-aware-training","title":"Quantization-Aware Training","text":"<p>Solution: Quantization-aware training (QAT) is a technique that simulates the effects of quantization during the training process, allowing the model to learn to adapt to the quantization noise.</p> Year Venue Authors Title Tags P E N 2018 arXiv IBM PACT: Parameterized Clipping Activation for Quantized Neural Networks activation quantization scheme for finding the optimal quantization scale during training 3 4 3 2020 ICLR IBM Learned Step Size Quantization approximate the gradient to the quantizer step size; heuristic to bring the magnitude of step size updates into better balance with weight updates 3 4 3"},{"location":"software/algorithm/#dnn-compression","title":"DNN Compression","text":"<p>Solution: DNN compression aims to reduce the size and computational requirements of deep neural networks</p> Year Venue Authors Title Tags P E N 2016 ICLR Stanford Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding three stage pipeline: pruning, trained quantization and Huffman coding 4 4 4 2020 JSTSP Fraunhofer HHI DeepCABAC: A Universal Compression Algorithm for Deep Neural Networks identify set of priors in DNN; redefine CABAC's core scheme to capture priors 3 5 3"},{"location":"software/algorithm/#statistical-parameter-estimation","title":"Statistical Parameter Estimation","text":"<p>Solution: infer the distribution of variables using statistical methods from observed data</p> Year Venue Authors Title Tags P E N 1977 JRSSB Harvard Maximum Likelihood from Incomplete Data via the EM Algorithm incomplete data; maximum likelihood expectation algorithm 2 1 3 2016 Big Data LPNU Machine Learning, Linear and Bayesian Models for Logistic Regression in Failure Detection Problems extreme gradient boosting classifier; generalized linear model 2 1 2 2023 J Process Contr UA Modeling and Bayesian inference for processes characterized by abrupt variations dynamic latent variable model; variational Bayesian inference framework 3 2 2"},{"location":"software/algorithm/#data-structures","title":"Data structures","text":"<p>Solution: organizing and storing data efficiently to enable fast access, modification, and processing</p>"},{"location":"software/algorithm/#dynamic-graph-processing","title":"Dynamic Graph Processing","text":"<p>Solution: data structures for processing dynamic graphs, which are graphs that change over time.</p>"},{"location":"software/algorithm/#architecture-specific-data-structures","title":"Architecture-specific Data Structures","text":"<p>Solution: Data structures targeting specific hardware architectures</p> Year Venue Authors Title Tags P E N 2023 TKDE PKU An Efficient Data Structure for Dynamic Graph on GPUs leveled packed memory array; redundancy-free top-down re-balancing method; con-concurrent strategy Opera 4 4 3 2024 VLDB PKU Towards Sufficient GPU-Accelerated Dynamic Graph Management: Survey and Experiment topology structure; attribute storage; auxiliary structures 4 4 2"},{"location":"software/algorithm/#computational-complexity","title":"Computational complexity","text":"<p>Solution: analyzing and classifying how the time and space requirements of an algorithm grow as the input size increases.</p>"},{"location":"software/algorithm/#computability-theory","title":"Computability theory","text":"<p>Solution: helping to identify the fundamental limits of what can be computed, regardless of time or space constraints.</p>"},{"location":"software/ds/","title":"Distributed Systems","text":""},{"location":"software/ds/#distributed-algorithms","title":"Distributed algorithms","text":"<p>Focusing on the distributed algorithms, such as consensus and replication, like RAFT.</p> <p>Challenge: concurrency, synchronous and communication complexities across independent nodes</p> <p>Solution: problems that require coordination, computation, and data management across multiple independent computer systems.</p>"},{"location":"software/ds/#computing-framework","title":"Computing Framework","text":"<p>Solution: Developing distributed algorithms requires a clear understanding of the computing framework, which scales small computing units to achieve a more clear data processing. The common computing frameworks are MapReduce, Spark, etc.</p> Year Venue Authors Title Tags P E N 2004 OSDI Google MapReduce: simplified data processing on large clusters divife the data processing into map and reduce stages; use master-worker architecture 4 5 5"},{"location":"software/ds/#domain-specific-computing-framework","title":"Domain Specific Computing Framework","text":"<p>Challenge: specific bounds of different situations</p> Year Venue Authors Title Tags P E N 2024 PPoPP NUDT GraphCube: Interconnection Hierarchy-aware Graph Processing interconnection hierarchy-aware; topology-aware graph partitioning; extreme-scale graph processing 4 5 5"},{"location":"software/ds/#parallel-strategies","title":"Parallel Strategies","text":"<p>Soultion: using the computation and memory resources of multiple processors to solve a problem.</p> <p>Challenge: communication overhead and load balancing</p>"},{"location":"software/ds/#data-parallelism","title":"Data Parallelism","text":"<p>Solution: Data parallelism addresses scenarios where a single GPU can accommodate the model, but the dataset's size necessitates distribution across multiple GPUs for efficient processing and accelerated training.</p> <p>Modern DNN acceleration systems commonly use the combination of data parallelism and model parallelism.</p> Year Venue Authors Title Tags P E N 2012 Nips Google Large Scale Distributed Deep Networks data parallel; use many model to optimize the same data; distributed model training 3 4 3 2014 OSDI CMU Scaling Distributed Machine Learning with the Parameter Server the foundation of tensor parallel; parameter server; pull-based data transfer 4 5 3 2020 SC Miscrosoft ZeRO: Memory Optimizations Toward Training Trillion Parameter Models fix the problem that dp cannot reduce the memory usage on single GPU 3 4 3"},{"location":"software/ds/#model-parallelism","title":"Model Parallelism","text":"<p>Solution: Model parallelism addresses scenarios where the model's size exceeds the processing and memory capacity of a single GPU. There are two types of model parallelism:</p> <ol> <li> <p>Pipeline parallelism: divide the model as pipeline stages, each gpu processes one or more stages.</p> </li> <li> <p>Tensor parallelism: divide the tensor into different GPUs.</p> </li> </ol> <p>Usually, pipeline parallelism and tensor parallelism are used together.</p> Year Venue Authors Title Tags P E N 2019 arXiv NVIDIA Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism transformer based model parallel; pipeline parallel; divide model into different GPUs 3 4 3 2021 SC NVIDIA Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM Megatron2; dive deep into tensor parallelism; how to train a LLM on 1000 GPUs 4 4 3 2022 arXiv NVIDIA Reducing Activation Recomputation in Large Transformer Models Megatron3; sequence parallel; selective activation recomputation; reduce the amount of recomputed activation 3 4 3"},{"location":"software/ds/#llm-specific-parallel-strategies","title":"LLM-specific Parallel Strategies","text":"<p>Focusing on the parallel strategies for LLM-specific deep learning systems.</p> Year Venue Authors Title Tags P E N 2022 ACL NUS Sequence Parallelism: Long Sequence Training from System Perspective splits input sequences into chunks; Ring Self-Attention; sparse attention 3 4 3"},{"location":"software/ds/#cloud-computing-platforms-and-architectures","title":"Cloud computing platforms and architectures","text":"<p>Challenge: when providing services to users, facing scalability, resource management, fault tolerance, and cost-effectiveness for building and deploying large-scale distributed applications and services.</p>"},{"location":"software/ds/#cloud-platform-llm-scheduling","title":"Cloud Platform LLM Scheduling","text":"<p>Challenge: meet the SLO when providing LLM service on cloud platform.</p> Year Venue Authors Title Tags P E N 2025 arXiv Azure TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms thermal/power property characterization; dynamically adjust in response to power or cooling failures; thermal- and poweraware manner"},{"location":"software/ds/#microservices","title":"Microservices","text":"<p>Focusing on the microservices.</p>"},{"location":"software/ds/#memory-management","title":"Memory Management","text":"<p>Challenge: coordinating memory access and maintaining data consistency across multiple independent nodes with their own local memories, especially when dealing with shared data.</p>"},{"location":"software/ds/#remote-memory","title":"Remote Memory","text":"<p>Challenge: efficiently providing access to memory on a remote node while minimizing latency and overhead, and ensuring consistency and reliability despite network communication complexities and potential failures.</p> Year Venue Authors Title Tags P E N 2020 TC Georgia Tech Hierarchical Orchestration of Disaggregated Memory XMemPod architecture for hierarchical memory orchestration; compressed swap page table (CSPT) for metadata management; hybrid swap-out algorithm for memory utilization; proactive swap-in optimization for performance; RDMA-based remote memory sharing for low-latency access"},{"location":"software/ds/#scratchpad-memory","title":"Scratchpad Memory","text":"<p>Challenge: efficiently allocating and coordinating limited fast memory across distributed nodes to minimize access latency and contention, while ensuring data consistency and scalability.</p> Year Venue Authors Title Tags P E N 2023 ASPLOS Cornell Beyond Static Parallel Loops: Supporting Dynamic Task Parallelism on Manycore Architectures with Software-Managed Scratchpad Memories work-stealing based dynamic task parallelism; stack/task queue in SPM; read-only data duplication 3 3 3"},{"location":"software/ds/#memory-optimization-for-graph-processing","title":"Memory Optimization for Graph Processing","text":"<p>Challenge: efficiently optimize huge memory requirement from graph processing.</p> Year Venue Authors Title Tags P E N 2024 PPoPP KAIST INFINEL: An efficient GPU-based processing method for unpredictable large output graph queries unpredictable large output queries; one-phase GPU graph processing; kernel stop/restart 4 4 3"},{"location":"software/ds/#llm-memory-management","title":"LLM Memory Management","text":"<p>Solution: efficient memory management can reduce memory usage, thus enable larger batch size and higher throughput.</p>"},{"location":"software/ds/#memory-management-algorithms","title":"Memory Management Algorithms","text":"<p>Solution: efficient memory management algorithms, like virtual memory, page table, etc.</p> Year Venue Authors Title Tags P E N 2023 SOSP UCB Efficient Memory Management for Large Language Model Serving with PagedAttention Paged KV-Cache management; Better memory management for larger batch size; Preemptive memory scheduling 4 5 3 2025 ASPLOS Miscrosoft vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention use cuda hardware page table instead of vllm's; hack cuda's driver to support page table modify 2 3 3 2025 arXiv SJTU eLLM: Elastic Memory Management Framework for Efficient LLM Serving activation weight paged; all scene virtual memory; cpu memory swap 3 2 2 2022 NIPS Stanford FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Generalized Acceleration of Attention Mechanisms; Change attention to utilize the SRAM on GPU; use recompute to reduce IO burden 4 5 4"},{"location":"software/ds/#general-llm-memory-management","title":"General LLM Memory Management","text":"<p>Challenge: LLM memory management faces challenges like limited HBM memory, efficient KV Cache management, memory sharing between multiple GPUs, multi-level memory management.</p> Year Venue Authors Title Tags P E N 2025 arXiv THU Jenga: Effective Memory Management for Serving LLM with Heterogeneity fixed-size embeddings; full-prefix dependency; two-level memory allocator 4 4 3 2025 FAST THU Mooncake: Trading More Storage for Less Computation \u2014 A KVCache-centric Architecture for Serving LLM Chatbot PD-disaggregate system; kv-cache centered; global kv-cache pool; dynamic SLO scheduler; paged KV-Cache storage 3 4 2 2022 SC Miscrosoft DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale kernel fusion; GPU-CPU-NVMe heterogeneous memory; PCIe-based memory prefetch 4 4 3"},{"location":"software/ds/#kv-cache-reuse-systems","title":"KV Cache Reuse Systems","text":"<p>Solution: reduce redundant computation and high memory consumption during inference by allowing the reuse of previously computed key-value pairs for shared or repeated parts of input sequences.</p>"},{"location":"software/ds/#prefix-sharing","title":"Prefix Sharing","text":"<p>Solution: reuse KV Cache when the input sequence has shared or repeated parts, use prefix tree to manage KV Cache.</p> Year Venue Authors Title Tags P E N 2023 Nips Stanford SGLang: Efficient Execution of Structured Language Model Programs KV-Cache share; python-like DSL; compute graph; LRU cache management stragety 4 4 3 2024 ACL Microsoft ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition prefix aware attention compute; manage kv-cache chunks as prefix tree; reduce kv-cache redundancy 3 4 2 2024 arXiv Microsoft BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching global prefix tree ahead-of-time; request reorder; horizontal fusioned prefix-shared attention kernel 2024 arXiv Berkeley BlendServe: Optimizing Offline Inference for Auto-regressive Large Models with Resource-aware Batching offline batch inference; resource-aware prefix tree; compute-intensive / memory-intensive requests 2024 arXiv UChicago DroidSpeak: Enhancing Cross-LLM Communication selectively layer reuse; communication protocol for inter-agent exchanges; LLMs that share a common foundational model"},{"location":"software/ds/#kv-cache-store","title":"KV cache store","text":"<p>Solution: store the KV cache in the memory or other storage device, supporting multi-level storage.</p> Year Venue Authors Title Tags P E N 2024 ATC Huawei Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention store KV cache in the memory; multi level KV cache management; position mask modified 3 3 3 2024 SIGCOMM UChicago CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving efficient KV Cache streaming; KV Cache compression; knowledge delivery network; The transfer part of LMCache 3 4 3 2024 EuroSys UChicago CacheBlend: Fast Large Language Model Serving for RAG with  Cached Knowledge Fusion multiple precomputed text chunks; selective KV recompute; sparsity of attention matrices; The system intro of LMCache 3 4 3"},{"location":"software/ds/#other-techniques","title":"Other Techniques","text":"<p>Solution: KV cache reuse techniques beyond prefix sharing. Prefix is a high requirement and is not always possible.</p> Year Venue Authors Title Tags P E N 2024 arXiv Berkeley Optimizing LLM Queries in Relational Workloads prefix sharing maximization; KV cache hit rate; deduplication and cost estimation techniques"},{"location":"software/ds/#kv-cache-storage-systems","title":"KV Cache Storage Systems","text":"<p>Solution: efficiently storing and retrieving the key-value cache, thus reuse when needed.</p> <p>Challenge: the prefetch and eviction of the KV cache, the balance between saving GPU memory and refetching time from the storage device.</p> Year Venue Authors Title Tags P E N 2025 arXiv NVIDIA FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving block-sparse format; customizable attention template; dynamic load-balanced scheduling framework 2025 arXiv PKU FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference imbalanced KV cache compression mitigation; fair-copying for load balancing; best-effort assignment"},{"location":"software/ds/#kv-cache-evict-systems","title":"KV Cache Evict Systems","text":"<p>Challenge: selectively discard the least important key-value pairs to free up memory for longer contexts or larger batch sizes without significantly degrading the model's generation quality or increasing computational overhead for the eviction process itself.</p> Year Venue Authors Title Tags P E N 2023 NIPS UT-Austin H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models sparsity for small cache size; heavy-hitters; greedy algorithm for low-cost policy 2024 arXiv Fujitsu CO2: Precise Attention Score Observation for improving KV Cache Replacement in Large Language Models long measurement step; decay of the accumulated attention score; adjusting FIFO cache size"},{"location":"software/ds/#systems-with-other-caches","title":"Systems with Other Caches","text":"<p>Solution: use other caches (not just KV cache) to improve the performance of LLM inference.</p> Year Venue Authors Title Tags P E N 2025 arXiv KAIST Efficient LLM Inference with Activation Checkpointing and Hybrid Caching activation checkpointing; KV-activation hybrid caching; balanced approach to determine the best ratio"},{"location":"software/ds/#llm-prefetching","title":"LLM Prefetching","text":"<p>Solution: prefetch to avoid memory transfer between different devices, reduce the latency of memory access.</p> Year Venue Authors Title Tags P E N 2025 arXiv Huawei Zurich PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving computational graph-based prefetching; prefetch KV cache to L2 cache"},{"location":"software/ds/#communication-centric-optimization","title":"Communication-Centric Optimization","text":"<p>Challenge: communication is a bottleneck of some distributed systems, trying to reduce the communication.</p>"},{"location":"software/ds/#io-characterization-and-optimization","title":"I/O Characterization and Optimization","text":"<p>Challenge: minimize data movement and maximize resource utilization across heterogeneous distributed environments.</p> Year Venue Authors Title Tags P E N 2020 ASPLOS CMU Livia: Data-Centric Computing Throughout the Memory Hierarchy Memory service programming model; task graphs linked to data location; dynamic task/data scheduling for minimal movement 2 4 3 2025 arXiv UOregon Parallel I/O Characterization and Optimization on Large-Scale HPC Systems: A 360-Degree Survey different HPC I/O stack layers; profiling and tracing tools; tuning echniques"},{"location":"software/ds/#gpu-gpu-communication","title":"GPU-GPU Communication","text":"<p>Challenge: limited interconnect bandwidth between GPUs using nvLink, PCIe, synchronization delays in parallel workloads, load imbalance across GPUs</p> Year Venue Authors Title Tags P E N 2025 arXiv Apple SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models sync-point drop; block-wise sensitivity analysis; attention output synchronization reduction"},{"location":"software/ds/#many-core-systems","title":"Many-Core Systems","text":"<p>Challenge: the heterogeneity of cores, the load imbalance, and the communication overhead.</p>"},{"location":"software/ds/#workload-characterization","title":"Workload Characterization","text":"<p>Challenge: dynamic workloads across numerous cores, resource contention for shared hardware.</p> Year Venue Authors Title Tags P E N 2015 VLDB Intel GraphMat: High performance graph analytics made productive vertex program to sparse matrix mapping; generalized SPMV for graph analytics; single-node multicore framework 4 4 4 2018 SC Intel Many-Core Graph Workload Analysis multicore simulator sniper; selective caching and prefetching; heterogeneous high-performance low-power cores 2018 DATE UGA Parallel Code Generation of Synchronous Programs for a Many-core Architecture banked memory mapping; worst-case response time analysis 2025 IPDPS UChicago Optimizing Fine-Grained Parallelism Through Dynamic Load Balancing on Multi-Socket Many-Core Systems lock-less and concurrent task queue xqueue; distributed tree barrier; NUMA-aware redirect push/work stealing"},{"location":"software/ds/#fault-propagation","title":"Fault Propagation","text":"<p>Challenge: one core or component can easily spread to others due to shared resources, leading to system-wide reliability issues. Core counts grow make it hard to predict, detect, and contain errors effectively.</p> Year Venue Authors Title Tags P E N 2008 ASPLOS UIUC Understanding the Propagation of Hard Errors to Software and Implications for Resilient System Design stuck-at fault; bridging fault; software failure detection 2010 PRDC UBC Modeling the Propagation of Intermittent Hardware Faults in Programs instruction based intermittent fault; dynamic dependency graph(DDG) based propagation modeling 2015 SC IBM Understanding the Propagation of Transient Errors in HPC Applications fault propagation in MPI application; fault classification:V,ONA,WO,PEX,C; fault propagation speed factors 2023 ISCA UChicago Understanding and Mitigating Hardware Failures in Deep Learning Training Accelerator Systems NVDLA based fault injection framework; re-execution based light-weight recovery technique; failure effects:SlowDegrade,SharpSlowDegrade,SharpDegrade,LowTestAccuracy"},{"location":"software/ds/#fault-injection-technique","title":"Fault Injection Technique","text":"<p>Challenge: It is difficult to target specific components, reproduce realistic fault scenarios, and observe system behavior without disturbing normal operation, especially as system scale and complexity increase.</p> Year Venue Authors Title Tags P E N 2008 VLSI DISCA Enhancement of Fault Injection Techniques Based on the Modification of VHDL Code saboteurs and mutants technique based fault injection; VHDL level fault-tolerance mechanism 2014 DSN UBC Quantifying the Accuracy of High-Level Fault Injection Techniques for Hardware Faults fault injection quantification; assembly level fault injection; LLVM compiler based fault injector"},{"location":"software/ds/#communication","title":"Communication","text":"<p>Challenge: efficiently managing data exchange between a large number of cores, due to limited bandwidth, high latency, and contention in shared resources like interconnects and memory.</p> Year Venue Authors Title Tags P E N 2025 arXiv UCLM Understanding intra-node communication in HPC systems and Datacenters intra- and inter-node simulation model; intra-node network interface bottleneck; impacts of communication pattern"},{"location":"software/ds/#heterogeneous-systems","title":"Heterogeneous Systems","text":"<p>Heterogeneous systems are systems that have different types of processors, such as CPUs and GPUs.</p> <p>Solution: ultilize the heterogeneous resources to improve the performance.</p>"},{"location":"software/ds/#general-applications","title":"General Applications","text":"Year Venue Authors Title Tags P E N 2013 SOSP MSR Silicon Valley Dandelion: a Compiler and Runtime for Heterogeneous  Systems unified programming model; \u201csingle machine\u201d abstraction; a rich object-oriented programming language for data-parallel computing 2025 EuroSys SJTU Improving GPU Sharing Performance through Adaptive Bubbleless Spatial-Temporal Sharing Bubble-less spatial-temporal sharing; kernel squad scheduling; fine-grained concurrent kernel management 4 3 2 2025 ISPASS CMU Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures Effective regions for balanced utilization of PUs; Proximity-based kernel fusion recommendation; operator-kernel dependency graphs from PyTorch Profiler traces 3 4 2"},{"location":"software/ds/#decentralized-serving","title":"Decentralized Serving","text":"<p>Challenge: managing diverse hardware and software environments, balancing workloads across uneven resources, minimizing communication overhead, ensuring consistency without centralized control.</p> Year Venue Authors Title Tags P E N 2019 ASPLOS USC Hop: Heterogeneity-aware Decentralized Training iteration gap; queue-based synchronization; backup workers and bounded staleness 2020 ASPLOS USC Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training Partial All-Reduce to reduce synchronization cost; group scheduling to avoid conflicts 2025 arXiv Berkeley DeServe: Towards Affordable Offline LLM Inference via Decentralization decentralized LLM inference; high-latency optimization; idle GPU utilization; modular on-chain integration 2025 arXiv HKUST DreamDDP: Accelerating Data Parallel Distributed LLM Training with Layer-wise Scheduled Partial Synchronization partial synchronization based local SGD; DFS algorithm with pruned search space; enables the opportunity of overlapping communication and computation"},{"location":"software/ds/#ml-training-systems","title":"ML Training Systems","text":"<p>Solution: balance between faster training and high precision.</p> Year Venue Authors Title Tags P E N 2023 SOSP CMU Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling heterogeneity-aware and adaptivity-aware; ILP formulation for scheduling; bootstrapped from observing just a few mini-batches"},{"location":"software/ds/#llm-inference-heterogeneous-systems","title":"LLM Inference Heterogeneous Systems","text":"<p>Solution: managing diverse hardware and software environments, balancing workloads across uneven resources, meeting the SLO.</p>"},{"location":"software/ds/#mobile-edge-network-serving","title":"Mobile &amp; Edge-Network Serving","text":"<p>Challenge: limited computation, memory, power coupled with intermittent and unreliable network connectivity, making it difficult to perform computationally intensive training tasks, manage large datasets, and ensure efficient communication and synchronization across distributed edge nodes.</p> Year Venue Authors Title Tags P E N 2024 arXiv UIC Priority-Aware Model-Distributed Inference at Edge Networks priority-aware model distributed inference algorithm; prioritization of ML inference tasks; model-distributed inferencing mechanism 2024 arXiv Yonsei Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models hybrid language model; selectively skip uplink transmissions; uncertainty-aware 2024 arXiv UMD Distributed Mixture-of-Agents for Edge Inference with Large Language Models Mixture-of-Agents; semantics of the data being gossiped and its timeliness; queuing stability 2025 arXiv PKU SplitLLM: Hierarchical Split Learning for Large Language Model over Wireless Network hierarchical split learning; edge-cloud collaboration; LoRA adapter update 2025 arXiv SJTU HeteroLLM: Accelerating Large Language Model Inference on Mobile SoCs platform with Heterogeneous AI Accelerators both layer-level and tensor-level GPU-NPU parallelism; different tensor partition strategies; fast synchronization mechanism based on predictable kernel waiting times; tensor partition solver"},{"location":"software/ds/#gpu-gpu-heterogeneous-system","title":"GPU-GPU Heterogeneous System","text":"<p>Solution: the system is composed of heterogeneous GPUs and not inferencing on CPU/ The system need to manage the heterogeneous GPUs' communication and memory.</p> Year Venue Authors Title Tags P E N 2024 arXiv CMU Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs LLM model placement as a max-flow problem; per-request pipeline; mixed integer linear programming 2025 ICLR HKUST HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment a combination of graph partitioning and max-flow algorithm; TP and PP with disaggregation; bottleneck and underutilized edges; swap edges"},{"location":"software/ds/#xpu-gpu-heterogeneous-system","title":"XPU-GPU Heterogeneous System","text":"<p>Challenge: effectively managing and coordinating diverse hardware (CPUs, TPUs, etc.), interconnects, and memory hierarchies</p> Year Venue Authors Title Tags P E N 2023 ICML Stanford FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU dynamic offload tensor; quantize the weights to 4-bits; linear aggregation of the store and load operations 4 4 3 2025 arXiv CMU Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures SKIP profiling tool; TKLQT metric for CPU/GPU boundedness; proximity score kernel fusion 2 3 2 2025 SPAA Huawei WindVE: Collaborative CPU-NPU Vector Embedding seamless CPU-NPU collaboration for vector embedding; linear regression based estimator; high-throughput offloading vector embedding 2 4 3 2025 arXiv Huawei High-Throughput LLM inference on Heterogeneous Clusters lightweight profiling while avoiding resource-intensive throughput benchmarks; a scheduler that accounts for both instance computational capacity and memory usage; exhaustive search method 2 4 2 2025 ISCA KAIST EOD: Enabling Low Latency GNN Inference via Near-Memory Concatenate Aggregation concatenated ZVC compression; precomputation for neighborhood explosion problem 2 3 2"},{"location":"software/ds/#heterogeneous-device-task-scheduling","title":"Heterogeneous Device Task Scheduling","text":"<p>Solution: assigning different parts of the LLM serving workload to the most suitable heterogeneous devices to maximize throughput and minimize latency.</p> Year Venue Authors Title Tags P E N 2023 PACT Yonsei Virtual PIM: Resource-aware Dynamic DPU Allocation and Workload Scheduling Framework for Multi-DPU PIM Architecture dynamic DPU allocation for multitasking; fine-grained scheduling 3 2 2 2025 arXiv NUS Data-aware Dynamic Execution of Irregular Workloads on Heterogeneous Systems lightweight and input-aware framework; multiobjective and multi-constraint design space; dynamically creating optimal schedules 2025 HPCA Samsung PAISE: PIM-Accelerated Inference Scheduling Engine for Transformer-based LLM task scheduling algorithm across host and PIM; interleave-batched GEMM; data layout adjustment 2 3 3"},{"location":"software/ds/#task-scheduling-for-specific-tasks","title":"Task Scheduling for specific tasks","text":"<p>Solution: In specific scene, the schedule goal is different. Assigning tasks to differnet devices can fix the gap between the characteristics of devices' and tasks'.</p> Year Venue Authors Title Tags P E N 2023 HPCA Princeton Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications distributed data-local tiled architecture; task-based programming for pointer indirection; traffic-aware task scheduling with headerless NoC 3 3 3 2025 arXiv Georgia Tech HARP: A Taxonomy for Heterogeneous and Hierarchical Processors for Mixed-reuse Workloads a taxonomy to classify the heterogeneous and hierarchical accelerators; characterize hardware organization of different accelerators; classify based on relative location of sub-accelerators 2025 arXiv PKU Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC agent application-specific scheduling on heterogeneous SoC; heterogeneous execution graph with eastic kernels; bandwidth-aware dispatch for NPU-iGPU contention mitigation 3 2 3"},{"location":"software/ds/#llm-training-heterogeneous-systems","title":"LLM Training Heterogeneous Systems","text":"<p>Solution: compared to LLM Inference Heterogeneous Systems, need to solve the backward compatibility and heterogeneity issues.</p> Year Venue Authors Title Tags P E N 2024 arXiv PKU Demystifying Workload Imbalances in Large Transformer Model Training over Variable-length Sequences data sampling imbalance; data packing imbalance; subgraph abstraction 2024 arXiv Ant Group EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models Local Stochastic Gradient Descent (Local SGD); consistent stragglers within heterogeneous devices; hierarchical distribution strategy on a two-dimensional device mesh; layer by layer forward syncing; pseudo-gradient penalty method 2024 arXiv ZJU Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters efficient and low-overhead task-to-cluster scheduling; bin-packing algorithms; seamless and user-friendly 2025 arXiv OSU Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning low-bandwidth interconnects; three-level hierarchical partitioning strategy; improved hierarchical partitioning on top of ZeRO++ 2025 arXiv PKU Split Fine-Tuning for Large Language Models in Wireless Networks split fine-tuning; device and server partition; novel compression scheme and resource management algorithm 2025 arXiv Neuchatel SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks partial pipeline parallelism; stage skipping; path scheduling algorithm"},{"location":"software/ds/#schedule-optimization","title":"Schedule Optimization","text":"<p>Solution: develop task schedule algorithms, to achieve efficient overall system performance despite incomplete and evolving system state information.performance.</p>"},{"location":"software/ds/#general-task-scheduling","title":"General Task Scheduling","text":"<p>Solution: optimizing the allocation and execution of diverse and dynamic workloads.</p> Year Venue Authors Title Tags P E N 2019 NSDI MIT Shinjuku: Preemptive Scheduling for \u00b5second-scale Tail Latency preemptive scheduling; single-address space OS; hardware-supported virtualization 2021 SOSP UPenn When Idling is Ideal: Optimizing Tail-Latency for Heavy-Tailed Datacenter Workloads with Pers\u00e9phone reserve cores; non-conserving; request dispatching algorithm 2017 HPCA UGent Reliability-Aware Scheduling on Heterogeneous Multicore Processors core reliability characteristics difference; system soft error rate; sampling-based reliability-aware scheduling algorithm 2020 TCAD ASU Runtime Task Scheduling Using Imitation Learning for Heterogeneous Many-Core Systems offline Oracle optimizaion strategy; hierarchical imitation learning based scheduling; two-level scheduling"},{"location":"software/ds/#speculative-execution-non-llm","title":"Speculative Execution (Non-LLM)","text":"<p>Solution: balancing the potential performance gains from speculative executions, including accurately predicting outcomes, handling incorrect speculations and their side effects across multiple nodes.</p> <p>Refer to Speculative Execution for the speculative execution algorithms for LLM.</p> Year Venue Authors Title Tags P E N 2024 arXiv MSR Forerunner: Constraint-based Speculative Transaction Execution for Ethereum constraint-based speculative transaction execution; many-future nature; specialized fast-path program 2024 arXiv Politecnico di Milano Minimizing speculation overhead in a parallel recognizer for regular texts speculation overhead; chunk automaton; reduced-interface DFA"},{"location":"software/ds/#llm-related-scheduling","title":"LLM-Related Scheduling","text":"<p>Challenge: efficiently managing the immense computational and memory demands of training and inference across numerous interconnected devices, requiring sophisticated strategies to partition massive models.</p>"},{"location":"software/ds/#llm-request-scheduling","title":"LLM Request Scheduling","text":"<p>Solution: develop intelligent strategies to route requests, prioritize urgent or critical tasks, handle varying input lengths and complexities, manage resource contention to meet the SLO requirements.</p> Year Venue Authors Title Tags P E N 2024 arXiv UCSB Multi-Bin Batching for Increasing LLM Inference Throughput binning-based scheduling strategy; queueing-theoretical analysis; asymptotical throughput optimality 2024 arXiv Yale TimelyLLM: Segmented LLM Serving System for Time-sensitive Robotic Applications segmented generation; time-sensitive scheduling; latency-guided batch size selection 2025 arXiv MSRI Niyama : Breaking the Silos of LLM Inference Serving QoS-driven LLM inference serving system; co-scheduling requests with diverse QoS targets on a shared rather than siloed infrastructure; allows graceful service degradation during overload conditions; deadline slack; a hybrid prioritization and an eager relegation policy 4 4 3 2025 arXiv MIT Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints fluid dynamics approximation; Waiting for Accumulated Inference Threshold; a hierarchical framework comprising multiple segments 3 4 2 2025 arXiv PKU SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large Language Model Inference service-aware and latency-optimized scheduling algorithm; doubling budget (DB) scheduling algorithm; search-based placement algorithm 3 4 2"},{"location":"software/ds/#info-predict-scheduling","title":"Info Predict Scheduling","text":"<p>Challenge: The general schedule if for better batching and meeting the SLO requirements. By predicting the information of the requests, we can make the schedule more efficient.</p> Year Venue Authors Title Tags P E N 2023 Nips Harvard S3: Increasing GPU Utilization during Generative Inference for Higher Throughput predict the length of LLM request to fixed types; Orca based dynamic batching 3 2 3 2024 ASPLOS UIUC Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction length prediction; left time prediction; bert-based proxy model 4 3 2"},{"location":"software/ds/#llm-application-level-scheduling","title":"LLM Application-Level Scheduling","text":"<p>Solution: to optimize the end-to-end latency of the application, including the scheduling of the LLM instances.</p> Year Venue Authors Title Tags P E N 2024 OSDI SJTU Parrot: Efficient Serving of LLM-based Applications with Semantic Variable Semantic Variable; application-level information; LLM applications as first-class citizens 2024 OSDI CUHK Teola: Towards End-to-End Optimization of LLM-based Applications mismatch between request-level scheduling and end-to-end  application performance; primitive-level dataflow graph; two-tier scheduling mechanism 2024 arXiv Yext SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering constantly changing and sometimes adverse conditions; Dynamically Reconfigurable Horizontal Scaling Framework; dynamically adjust resource allocation based on query requirements 2025 arXiv Berkeley Autellix: An Efficient Serving Engine for LLM Agents as General Programs formalize agentic programs as dynamic, non-deterministic DAGs; non-clairvoyant scheduler; simple load-balancing policy to balance data locality and KV-cache recomputation 2025 ICDCS SJTU LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications a DAG with regular stage, LLM stage, dynamic stage; bayesian network-based profiler; identify uncertainty-reducing stages 4 4 3 2025 arXiv SJTU Efficient Serving of LLM Applications with Probabilistic Demand Modeling DAG-based scheduling; dynamic excution; cpu excutor warmup 3 1 1"},{"location":"software/ds/#llm-speculative-inference","title":"LLM Speculative Inference","text":"<p>Refer to non-LLM speculative execution.</p> Year Venue Authors Title Tags P E N 2024 arXiv F&amp;M College AMUSD: Asynchronous Multi-Device Speculative Decoding for LLM Acceleration simultaneous and independent predictions; asynchronous speculative decoding; rollback mechanism 2024 arXiv Purdue Constrained Decoding with Speculative Lookaheads computational expense of generating lookaheads; speculated lookaheads; task specific reward function 2024 arXiv Rutgers Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface active user intervention; speculative planning algorithm; UI-level rescheduling algorithm 2024 arXiv USTC Parallel Speculative Decoding with Adaptive Draft Length adaptive draft length; pre-verify and post-verify; draft-then-verify framework; mutual waiting problem 2024 arXiv SEU SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative Decoding reasoning tree construction; parallel drafting with speculative decoding; FCFS queue verification"},{"location":"software/ds/#spec-others","title":"Spec + Others","text":"Year Venue Authors Title Tags P E N 2025 arXiv Huawei Speculative MoE: Communication Efficient Parallel MoE Inference with Speculative Token and Expert Pre-scheduling speculative MoE; speculative token shuffling; speculative expert pre-grouping 2025 INFOCOM UoA SPIN: Accelerating Large Language Model Inference with Heterogeneous Speculative Models internal neurons sparsification; model-agnostic acceleration framework; dynamic early-exit thresholds; multi-layered feature fusion 2025 arXic SUST FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference SPEC on memory limited dedvices; Efficient draft management with tree pruning and early stop reduces redundancy and maintains causal relationships 3 3 3"},{"location":"software/ds/#llm-serving-outages-and-incidents","title":"LLM Serving Outages and Incidents","text":"Year Venue Authors Title Tags P E N 2025 arXiv Vrije Universiteit Amsterdam An Empirical Characterization of Outages and Incidents in Public Services for Large Language Models empirical characterization of outages; failure recovery optimization; public LLM service reliability"},{"location":"software/ds/#energy-optimized-llm-scheduling","title":"Energy-Optimized LLM Scheduling","text":"Year Venue Authors Title Tags P E N 2025 arXiv UvA GREEN-CODE: Optimizing Energy Efficiency in Large Language Models for Code Generation dynamic early exit; energy-aware code generation; reinforcement learning for llms"},{"location":"software/ds/#dnn-scheduling","title":"DNN Scheduling","text":"<p>Solution: optimizing data parallelism and model parallelism while minimizing communication overhead between nodes, effectively managing limited GPU memory and other resources to achieve scalability and high throughput.</p> <p>Refer to LLM-Related Scheduling for the LLM-related scheduling algorithms.</p>"},{"location":"software/ds/#task-offloading","title":"Task Offloading","text":"Year Venue Authors Title Tags P E N 2024 arXiv USTC Collaborative Inference for Large Models with Task Offloading and Early Exiting early exit mechanism; jointly optimize its offloading strategy and the confidence threshold; distributed task offloading algorithm 2025 ISCA ETHZ OptiPIM: Optimizing Processing-in-Memory Acceleration Using Integer Linear Programming integer linear programming for offload optimization; PIM-friendly mapping representation; accurate cost modeling for data layout 4 2 3"},{"location":"software/ds/#general-optimizations-for-deep-learning-systems","title":"General optimizations for Deep Learning Systems","text":"<p>Solution: general optimizations for deep learning systems.</p> <p>If the paper is focusing on an above-mentioned specific scene (e.g., memory, scheduling, IO, etc.), it will be put in the corresponding section.</p>"},{"location":"software/ds/#llm-training-systems","title":"LLM Training Systems","text":"<p>Solution: arrange model parameters and data across multiple devices, reduce the time spent communicating, scale up smoothly as models and data keep growing\u2014all while staying efficient and speeding up training.</p>"},{"location":"software/ds/#general-optimizations","title":"General Optimizations","text":"Year Venue Authors Title Tags P E N 2025 arXiv THU Enhancing Memory Efficiency in Large Language Model Training Through Chronos-aware Pipeline Parallelism chronos-aware pipeline parallelism; temporal locality optimization; activation balancing 2025 arXiv NUS PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization selective offload strategy; memory offload optimization; pipeline parallelism scalability; lifespan-based offloading 2025 arXiv UCSD WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training workload-aware variable-length document packing; per-document sharding strategy; adaptive sharding selection mechanism; delay execution of extremely long documents 4 5 2 2025 EuroSys UToronto Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization fine-grained overlap-centric scheduling; symbolic-based performance analysis; imbalance-aware hierarchical tuning 4 4 2"},{"location":"software/ds/#optimizations-on-special-scene","title":"Optimizations on Special Scene","text":"Year Venue Authors Title Tags P E N 2025 arXiv HKU Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism Fully Sharded Sparse Data Parallelism (FSSDP); sparsely materializes MoE parameters; two sparse collective communications 2025 arXiv SJTU PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline dynamic interleaved pipeline; hierarchical schedule space for rapid pipeline schedule search; spatialtemporal subgraph reuse 3 4 2"},{"location":"software/ds/#experiments","title":"Experiments","text":"Year Venue Authors Title Tags P E N 2025 arXiv JSC Memory and Bandwidth are All You Need for Fully Sharded Data Parallel an extensive analysis of the FSDP training distribution strategy; a grid search methodology; both simulation and empirical results 2 4 1"},{"location":"software/ds/#multi-modal-optimizations","title":"Multi-Modal Optimizations","text":"<p>Challenge: multimodal data is more complex and requires more resources to train.</p> Year Venue Authors Title Tags P E N 2025 arXiv ByteDance OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training multimodal mini-batch imbalance; batch post-balancing algorithm; node-wise all-to-all communicator for practical rearrangement of mini-batches 4 4 3"},{"location":"software/ds/#kernel-level-optimizations","title":"Kernel-Level Optimizations","text":"Year Venue Authors Title Tags P E N 2025 arXiv HUST CFP: Low-overhead Profiling-based Intra-operator Parallelism Generation by Preserving Communication-Free Structures model segment profile-based cost model; communication-free tensor partition propagation property; extracting a set of unique model segments; Communication-Free Preserve 4 5 3"},{"location":"software/ds/#llm-inference-systems","title":"LLM Inference Systems","text":"<p>Focusing on the optimizations for LLM inference systems.</p> Year Venue Authors Title Tags P E N 2025 ISCA DeepSeek Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures software-hardware co-design for deepseek-v3; insight into hardware for ai architectures 5 5 4 2024 Mlsys SJTU FlashDecoding++: Faster Large Language Model Inference on GPUs asynchronized softmax with unified max value; flat GEMM optimization with double buffering; heuristic dataflow with hardware resource adaptation 4 4 3"},{"location":"software/ds/#slo-aware-systems","title":"SLO-Aware Systems","text":"<p>Challenge: providing service for users to meet specific latency requirements with limited resources.</p> Year Venue Authors Title Tags P E N 2025 arXiv Berkeley AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative Decoding fine-grained speculative decoding; token tree verification; slo customization 2025 arXiv UIUC HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location online-offline request co-location; interference-aware profiler; latency predictor; adaptive scheduler 2025 arXiv PKU Memory Offloading for Large Language Model Inference with Latency SLO Guarantees effectively captures the tension between meeting SLOs and maximizing host memory usage; dynamic offloading interval; per-bus coordinator 2025 arXiv Huawei Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization hybrid offline-online scheduling; preemptive scheduling for hardware utilization; lagrangian method for cost efficiency evaluation 2025 ASPLOS BUAA Past-Future Scheduler for LLM Serving under SLA Guarantees lightLLM; predict future system memory usage; reduce evict by better request scheduling 3 2 3"},{"location":"software/ds/#surveys","title":"Surveys","text":""},{"location":"software/ds/#system-optimization-surveys","title":"System Optimization Surveys","text":"Year Venue Authors Title Tags P E N 2024 arXiv NEU LLM Inference Serving: Survey of Recent Advances and Opportunities KV cache and memory management; LLM computation optimization; Cloud LLM deployment; focus on system-level enhancements 2024 arXiv CUHK A Survey on Inference Optimization Techniques for Mixture of Experts Models model compression; expert skip; expert merge; sparse to dense; expert parallel; expert offloading 2024 arXiv PolyU A Survey on Large Language Model Acceleration based on KV Cache Management cache selection; budget allocation; cache merging; cache quantization; cache low-rank decomposition; attention grouping and sharing; memory management; hardware-aware design 2025 arXiv THU Beyond A Single AI Cluster: A Survey of Decentralized LLM Training resource-driven paradigm; community-driven decentralization; organizational decentralization; decentralized LLM training taxonomy 2025 arXiv FIU Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions distributed solutions for LMs; workload imbalance in LLM training; M-ICL; model security enhancement"},{"location":"software/ds/#application-surveys","title":"Application Surveys","text":"Year Venue Authors Title Tags P E N 2024 arXiv PKU Retrieval-Augmented Generation for AI-Generated Content: A Survey Query Transformation; Data Augmentation; Recursive Retrieval; Chunk Optimization; Retriever Finetuning; Hybrid Retrieval; Re-ranking; Retrieval Transformation; Prompt Engineering; Decoding Tuning; Generator Finetuning; Output Rewrite; Adaptive Retrieval; Iterative RAG 2024 arXiv WHU A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges personalized characteristics; perceive environmental information; utilize memory mechanisms; mutual interaction; agent self-reflection 2024 arXiv PolyU Deploying Foundation Model Powered Agent Services: A Survey FM-powered agent services within the edge-cloud environment; low-level hardware perspective; high-level software perspective"},{"location":"software/ds/#multimodal-systems","title":"Multimodal Systems","text":"Year Venue Authors Title Tags P E N 2025 arXiv UW\u2013Madison LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models query-block distributed exchange; shared visual token recomputation; sequence-parallelism with minimal communication overhead 2025 arXiv Microsoft Towards Efficient Large Multimodal Model Serving fine-grained stage-aware resource management; multimodal workload-specific scheduling; model architecture-specific optimizations 2025 arXiv Huawei Efficiently Serving Large Multimedia Models Using EPD Disaggregation encode-prefill-decode disaggregation; multimodal cache; intra-request parallel 2025 arXiv TU/e Fine-tuning Multimodal Transformers on Edge: A Parallel Split Learning Approach Multimodal Parallel Split Learning; computation-efficient training; server-side loss aggregation mechanism 2025 arXiv HUST FastCache: Optimizing Multimodal LLM Serving through Lightweight KV-Cache Compression Framework resource-aware KV-cache memory pool; multimodal KV-cache compression; modality-specific compression"},{"location":"software/ds/#mixture-of-experts-llm-systems","title":"Mixture-of-Experts LLM Systems","text":"<p>Challenge: efficiently coordinating and scaling expert models across multiple nodes, leading to issues like uneven workload distribution, high communication overhead, and difficulty in fault tolerance.</p>"},{"location":"software/ds/#expert-offloading-and-placement","title":"Expert Offloading and Placement","text":"Year Venue Authors Title Tags P E N 2025 DATE Berkeley DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient MoE Inference data-aware offloading; predictive pre-calculation; sequence-specific expert allocation 2025 arXiv Stevens Tech fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving expert map; iteration-level probability distributions; track fine-grained input semantic embeddings; semantic-based and trajectorybased 2025 arXiv Georgia Tech MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing ILP for expert placement; cross-layer dependencies; minimizing total dispatched token number 2025 EuroMLSys EPFL Accelerating MoE Model Inference with Expert Sharding expert sharding for load balancing; tensor sharding for moe experts; fused expert computations for reduced kernel launches 2025 DAC PKU HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference dynamically balances workloads across GPUs and CPUs; impact-driven prefetching; MoE-specialized cache management 3 4 2"},{"location":"software/ds/#batching-and-scheduling","title":"Batching and Scheduling","text":"Year Venue Authors Title Tags P E N 2025 arXiv Alibaba Static Batching of Irregular Workloads on GPUs: Framework and Application to Efficient MoE Model Inference statically batching irregular workloads; batch-task-tile partition; decompress the mapping and dispatch the workload 2025 arXiv Edinburgh MoE-Gen: High-Throughput MoE Inference on a Single GPU with Module-Based Batching module-based batching; high-throughput MoE inference; full KV-cache offloading 2025 arXiv KTH Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference fine-grained preemption; priority-aware scheduling; per-expert queues; expert-level preemption 2025 arXiv UMich MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints two-stage performance modeling; analyzes the theoretical performance upper bound; captures how system execution mechanisms 4 4 2 2025 Arxiv Nvidia MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core decouples parallelization strategies for attention and MoE layers; flexible and efficient token-level dispatcher; 5-D hybrid parallelism 4 5 2"},{"location":"software/ds/#memory-and-communication-efficiency","title":"Memory and Communication Efficiency","text":"Year Venue Authors Title Tags P E N 2025 arXiv ByteDance Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts fine-grained communication-computation overlapping for efficient MoE execution; dependency resolving method; adaptive workload assignment method; shared data buffers between communication and computation operations 2025 arXiv UVA eMoE: Task-aware Memory Efficient Mixture-of-Experts-Based (MoE) Model Inference expert prediction; task-aware expert loading; task-aware request scheduling 2025 mobiCom HKUST D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving dually sparselygated Mixture-of-Experts; token-adaptive bit-width selection; matryoshka weight quantization; bit-width-aware I/O-compute pipeline 3 4 4 2025 ODSI SJTU Fast and Live Model Auto Scaling with O(1) Host Caching auto-scaling with minimal caching; optimize parameter loading; enabling fine-grained layer-level scale 3 3 2"},{"location":"software/ds/#architectural-innovations","title":"Architectural Innovations","text":"Year Venue Authors Title Tags P E N 2025 arXiv Shanghai AI Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts linear sequence modeling with MoE; sparse activation via moe layers; hybrid models combining linear-moe and transformer-moe layers 2025 arXiv Berkeley HeterMoE: Efficient Training of Mixture-of-Experts Models on Heterogeneous GPUs zebra parallelism; attention-expert disaggregation; asymmetric expert assignment mechanism; gather and squeeze strategy 4 5 3"},{"location":"software/ds/#compute-kernel-level-optimizations","title":"Compute-Kernel-Level Optimizations","text":"Year Venue Authors Title Tags P E N 2025 arXiv SJTU Samoyeds: Accelerating MoE Models with Structured Sparsity Leveraging Sparse Tensor Cores dual-side structured sparsity; sparse-sparse matrix multiplication kernel; vector-wise + 2:4 hybrid sparsity; token-aware activation compression"},{"location":"software/ds/#long-sequence-llm-systems","title":"Long Sequence LLM Systems","text":"Year Venue Authors Title Tags P E N 2024 OSDI SJTU &amp; Alibaba Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache inefficient model parallelism intra-instance; inefficient resource management inter-instance; KV cache scheduling 2025 arXiv PKU ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs hybrid data parallelism; data-aware sharding; a heuristic algorithm that reorganizes data assignment based on the characteristics of data and pipeline parallelism"},{"location":"software/ds/#sparse-attention","title":"Sparse Attention","text":"<p>Solution: handle the prompt token by token introduce high latency, trying to use sparse attention to reduce the computation and memory burden. This can be achieved by not using the full attention matrix, but only the upper triangular part.</p> Year Venue Authors Title Tags P E N 2025 arXiv CWRU Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques sparse attention with graph computing perspective; work-optimal graph algorithms; achieve true sparsity 2025 MLSys MIT LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention unified sparse attention; hybrid static and dynamic sparsity; hierarchical kv cache management with query-centric pruning"},{"location":"software/ds/#ring-computation","title":"Ring Computation","text":"<p>Solution: use the device layout to reduce the communication overhead. The key idea is to parallel the computation and communication.</p> Year Venue Authors Title Tags P E N 2023 Nips UCB Ring Attention with Blockwise Transformers for Near-Infinite Context divide the input into blocks and each block is processed by a single GPU; ring-type device layout 4 3 3 2024 arXiv SJTU TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication communication-oriented parallelism framework; inter-node P2P bidirectional communication bandwidth; optimization of attention block communication"},{"location":"software/ds/#p-d-disaggregated-systems","title":"P-D Disaggregated Systems","text":"Year Venue Authors Title Tags P E N 2024 OSDI PKU DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving goodput-optimized; prefill-decoding interference;novel placement algorithm for p-d schema 2024 ISCA UW Splitwise: Efficient Generative LLM Inference Using Phase Splitting optimized cache context transfer; performance per dollar; performance per watt; exploration of homogeneous and heterogeneous cluster deployments 2024 arXiv CMU A System for Microserving of LLMs fine-grained sub-request level actions; dynamic reconfiguration according to workloads; unified KV cache abstraction 2025 arXiv PKU ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments two-level hierarchical optimization; tabu search algorithm for GPU partition; a lightweight re-scheduling mechanism"},{"location":"software/ds/#p-d-disaggregated-system-optimizations","title":"P-D Disaggregated System Optimizations","text":"Year Venue Authors Title Tags P E N 2025 arXiv ByteDance KVDirect: Distributed Disaggregated LLM Inference tensor-centric communication mechanism; pull-based KV cache transfer; dynamic GPU resource scheduling via RDMA 2025 arXiv SYSU Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation attention disaggregation and offloading mechanism; low-latency decoding synchronization; resource-efficient prefill colocation; load-aware offloading scheduling 4 4 3 2025 arXiv Alibaba FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling analyze the communication patterns; KV cache structure adjustment method; load-aware scheduling 4 4 2 2025 arXiv NUS &amp; USTC DynaServe: Unified and Elastic Tandem-Style Execution for Dynamic Disaggregated LLM Serving a novel Tandem Serving execution model; two virtual subrequests; explicitly permit the two subrequests to execute on either GPU instance 3 4 2"},{"location":"software/ds/#throughput-optimized-systems","title":"Throughput-Optimized Systems","text":"Year Venue Authors Title Tags P E N 2025 arXiv HKUST Improving the End-to-End Efficiency of Offline Inference for Multi-LLM Applications Based on Sampling and Simulation sampling-then-simulation cost model; model-level pipeline parallelism; minimumtotal-latency application scheduling 4 4 3"},{"location":"software/ds/#fair-serving-systems","title":"Fair Serving Systems","text":"Year Venue Authors Title Tags P E N 2024 arXiv Virginia Tech Ensuring Fair LLM Serving Amid Diverse Applications multi-tenant LLM platform; overload and interaction-driven throttling; weighted service counter 2025 arXiv UIUC Hierarchical Autoscaling for Large Language Model Serving with Chiron hierarchical backpressure; interactive requests and batch requests; mixed instances 2025 arXiv Berkeley Locality-aware Fair Scheduling in LLM Serving deficit-based longest prefix matching; distributed deficit-round coordination; prefix-aware fairness bound analysis"},{"location":"software/ds/#rlhf-system","title":"RLHF System","text":"<p>Challenge: RLHF system includes both training and inference. On top of that, multi agents(LLMs) when running in parallel, which makes the data flow more complex.</p> Year Venue Authors Title Tags P E N 2025 EuroSys HKU HybridFlow: A Flexible and Efficient RLHF Framework auto-mapping model placement; 3D-HybridEngine to reduce the communication overhead; hybrid programming 4 4 3 2025 arXiv Alibaba Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library bind many LLMs in one device cluster; fix the batch problem of long tail requests; reuse many utils in HybridFlow 4 4 2"},{"location":"software/ds/#communication-computation-overlap","title":"Communication-Computation Overlap","text":"<p>Challenge: effectively hiding communication latency by overlapping it with computation, which requires careful scheduling and resource management to avoid bottlenecks and ensure that both communication and computation proceed efficiently without stalling each other.</p> Year Venue Authors Title Tags P E N 2023 NSDI KAIST ARK: GPU-driven Code Execution for Distributed Deep Learning communication-motivated DL system; pipeline DMA engine; GPU-direct-controlled DMA 2024 ASPLOS PKU Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning communication partition abstraction; hybrid LLM training tasks; 3-level decompose 2024 ASPLOS UW\u2013Madison T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives lightweight track and trigger; pre-programmed DMA commands; atomic memory update 2024 ASPLOS UIUC Two-Face: Combining Collective and One-Sided Communication for Efficient Distributed SpMM distributed SpMM; sparsity-aware partition; Synchronous Stripes and Asynchronous Stripes 2024 arXiv AMD Optimizing ML Concurrent Computation and Communication with GPU DMA Engines concurrent computation and communication; compute and memory interference among concurrent kernels; schedule prioritization and careful resource partitioning"},{"location":"software/ds/#configuration-optimization","title":"Configuration Optimization","text":"<p>Challenge: the configuration space is too large to be searched manually.</p> Year Venue Authors Title Tags P E N 2025 OSDI PKU Mirage: A Multi-Level Superoptimizer for Tensor Programs auto algebraically transfer tensor; using DAG to search configuration space; auto generate kernel function 4 4 3 2020 ASPLOS PKU FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System tvm auto schedule; RL based stragety find; auto optimizing in large configuration space 4 4 3"},{"location":"software/hpc/","title":"High-Performance Computing","text":""},{"location":"software/hpc/#parallel-programming-models","title":"Parallel programming models","text":"<p>Challenge: the complexity of great number of cores and the heterogeneity of the hardware</p>"},{"location":"software/hpc/#scientific-computing-applications-and-libraries","title":"Scientific computing applications and libraries","text":"<p>Solution: enable researchers to model physical phenomena, process large datasets, and accelerate discoveries across fields on HPC systems</p>"},{"location":"software/hpc/#parallel-algorithms","title":"Parallel algorithms","text":"<p>Solution: develop parallel algorithms for HPC systems, enabling scalable performance</p>"},{"location":"software/os/","title":"Operating Systems","text":""},{"location":"software/os/#kernel-design-and-implementation","title":"Kernel design and implementation","text":"<p>Solution: abstracting the underlying hardware and providing a secure, managed, and efficient environment for user-level applications to run concurrently and share system resources</p>"},{"location":"software/os/#process-and-thread-management","title":"Process and thread management","text":"<p>Solution: ensuring fair resource allocation, smooth multitasking, and proper coordination between processes and threads to maximize system performance and responsiveness.</p>"},{"location":"software/os/#memory-management","title":"Memory management","text":"<p>Memory management is the basic of system design.</p>"},{"location":"software/os/#memory-partitioning-mapping","title":"Memory Partitioning &amp; Mapping","text":"<p>Solution: efficiently and securely divide physical memory among multiple processes or systems</p> <p>Challenge: ensuring processes get enough memory without waste, while preventing interference and maintaining system performance and security.</p> Year Venue Authors Title Tags P E N 2012 HPCA Georgia Institute of Technology TAP: A TLP-Aware Cache Management Policy for a CPU-GPU Heterogeneous Architecture thread-level parallelism; core sampling for cache effort indentification; cache block lifetime normalization 2024 arXiv UMich Mercury: QoS-Aware Tiered Memory System contend for local memory; priority inversion; intra- and inter-tier interference; per-tier page reclaimation 2025 HPCA Seoul National FACIL: Flexible DRAM Address Mapping for SoC-PIM Cooperative On-device LLM Inference flexible PIM address mapping and remapping; OS paging mechanism extension for MapID; user-level mapping selector 3 2 3"},{"location":"software/os/#page-migration","title":"Page Migration","text":"<p>Challenge: the trade-off between migration overhead and potential performance gains, complicated by prediction accuracy and hardware complexities</p> Year Venue Authors Title Tags P E N 2024 SC THU Hydrogen: Contention-Aware Hybrid Memory for Heterogeneous CPU-GPU Architectures fast memory decoupled partitioning; token-based slow memory migration; epoch-based sampling method; consistent hashing based reconfiguration 2024 OSDI UT Arlington Nomad: Non-Exclusive Memory Tiering via Transactional Page Migration Memory allocator for hardware tiering to mitigate outliers 2025 ISCA XMU ArtMem: Adaptive Migration in Reinforcement Learning-Enabled Tiered Memory reinforcement learning for tiered memory management; dynamic adjustment of migration scope 2 4 3"},{"location":"software/os/#memory-pooling","title":"Memory Pooling","text":"<p>Solution: Memory Pooling reduces the overhead of frequent memory allocation and deallocation by maintaining a pool of pre-allocated memory blocks for reuse.</p> Year Venue Authors Title Tags P E N 2023 ASPLOS Virginia Tech Pond: CXL-Based Memory Pooling Systems for Cloud Platforms CXL-based memory pooling; small-pool design for low latency; machine learning model for memory allocation prediction; zero-core virtual NUMA (zNUMA) node for untouched memory"},{"location":"software/os/#cache-evict","title":"Cache Evict","text":"<p>Challenge: to balance the cache hit ratio and the cache miss penalty.</p> <p>The cache here not only includes the cache in the processors, but also the software concept in systems.</p> Year Venue Authors Title Tags P E N 2024 NSDI CMU Sieve is Simpler than LRU: an Efficient Turn-Key Eviction Algorithm for Web Caches web cache management; fifo-like schedule; recognizing the importance of objects 3 3 3"},{"location":"software/os/#file-systems-and-storage","title":"File systems and storage","text":"<p>Solution: organizing, storing, and retrieving data efficiently and reliably on storage devices, manage disk space, and ensure data integrity and security.</p>"},{"location":"software/os/#storage-systems","title":"Storage Systems","text":"<p>Solution: eficient, reliable, and secure management of data storage across diverse hardware while achieving balance between performance, durability, efficiency, and scalability .</p>"},{"location":"software/os/#ssd-management","title":"SSD Management","text":"<p>Challenge: limited write endurance, garbage collection, wear leveling, and the mismatch between erase and write operations, while optimizing performance and extending device lifespan</p> Year Venue Authors Title Tags P E N 2025 EuroSys Samsung Towards Efficient Flash Caches with Emerging NVMe Flexible Data Placement SSDs NVMe Flexible Data Placement (FDP) SSDs for data segregation; targeted data placement for reduced device write amplification; FDP-enabled CacheLib architecture; theoretical DLWA model for CacheLib 2025 arXiv SDU Managing Hybrid Solid-State Drives Using Large Language Models LLM-based auto-tuning framework for hybrid SSD management; hybrid SSD parameter categorization; performance-sensitive parameter selection; prompt engineering for LLM integration; dynamic configuration optimization in hybrid SSDs 2025 arXiv ETHZ Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems dual RL agents for data placement &amp; migration; I/O latency-based reward shaping 3 4 4"},{"location":"software/os/#virtualization","title":"Virtualization","text":"<p>Solution: efficiently managing and isolating multiple virtual machines (VMs) or containers on a single physical machine, allowing them to share hardware resources securely and independently while maintaining performance and flexibility.</p>"},{"location":"software/os/#synchronization","title":"Synchronization","text":"<p>Solution: coordinating access to shared resources among multiple processes or threads to prevent conflicts, ensure data consistency, and avoid issues like race conditions, deadlocks, and resource starvation.</p>"},{"location":"software/perf/","title":"Performance Analysis","text":""},{"location":"software/perf/#profilers-sampling-instrumentation","title":"Profilers (sampling, instrumentation)","text":""},{"location":"software/perf/#detection","title":"Detection","text":""},{"location":"software/perf/#bottleneck-analysis","title":"Bottleneck Analysis","text":"<p>Challenge: Bottleneck Analysis faces challenges of high system complexity, unexpected real-world factors and the resource constraints when detecting.</p> Year Venue Authors Title Tags P E N 2018 PPoPP THU vSensor: Leveraging Fixed-Workload Snippets of Programs for Performance Variance Detection fixed-workload snippets; dependency propagation algorithm; lightweight on-line analysis algorithm 2020 SC THU ScalAna: automating scaling loss detection with graph analysis program structure graph; program performance graph; backtracking root cause detection algorithm 2022 PPoPP THU Vapro: Performance Variance Detection and Diagnosis for Production-Run Parallel Applications state transition graph; fixed workload snippets identification clustering algorithm; variance breakdown model; time of factors quantification method 2024 arXiv UGA Performance Debugging through Microarchitectural Sensitivity and Causality Analysis constraints propagation engine for causality analysis; differential analysis engine for sensitivity analysis 2024 SC BUAA GVARP: Detecting Performance Variance on Large-Scale Heterogeneous Systems asynchronous state transition graph; parameter-based workload estimation method; asynchronous event tracing technology"},{"location":"software/perf/#variance-attribution","title":"Variance Attribution","text":"Year Venue Authors Title Tags P E N 2014 ISPASS Intel A Top-Down Method for Performance Analysis and Counters Architecture top-down bottleneck analysis method; frontend bound; bad speculation; retiring; backend bound 2016 TPDS ICT Understanding Big Data Analytics Workloads on Modern Processors top-down analysis for big data workload; pipeline-characteristics basd performance implication analysis; BigDataBench benchmark 2019 SC NCSU Pinpointing Performance Inefficiencies via Lightweight Variance Profiling function-level variance detection; stack based deep call chains maintain; on-the-fly binary analysis technique for calling context"},{"location":"software/perf/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>Challenge: difficulties in dependency graph modeling, scalability of detection algorithm for large-scale applications.</p> Year Venue Authors Title Tags P E N 2003 TISSEC IBM Clustering Intrusion Detection Alarms to Support Root Cause Analysis attribute-oriented induction based clustering algorithm; generalized alarm analysis 2 3 2 2017 Arxiv Intel; CA technologies Survey on Models and Techniques for Root-Cause Analysis deterministic/probabilistic model; RCA learning algorithms; RCA inference algorithms 4 1 1 2021 ASE eBay Groot: An Event-graph-based Approach for Root Cause Analysis in Industrial Settings event-graph based RCA; service dependency graph; event causality graph; pagerank based root cause ranking 4 5 2 2021 ASPLOS Cornell Sage: Practical &amp; Scalable ML-Driven Performance Debugging in Microservices RPC latency decomposition model; Markov based RPC latency propagation; causal bayesian network based dependency model 3 3 2 2023 ASPLOS Alibaba Sleuth: A Trace-Based Root Cause Analysis System for Large-Scale Microservices with Graph Neural Networks HDBSCAN trace clustering algorithm; GNN based dependency modeling 3 3 2"},{"location":"software/perf/#burst-detection","title":"Burst Detection","text":"<p>Challenge: maintaining accuracy at high speed data streams, tradeoff between memory usage and detection accuracy.</p>"},{"location":"software/perf/#heavy-hitter-burst","title":"Heavy Hitter Burst","text":"Year Venue Authors Title Tags P E N 2019 CloudNet PKU Dynamic Sketch: Efficient and Adjustable Heavy Hitter Detection for Software Packet Processing door keeper mechanism for high memory efficiency; bucket sampling for accuracy monitoring 3 3 1 2021 SIGMOD PKU BurstSketch: Finding Bursts in Data Streams running track based burst item filtering; snapshotting based burst item detection 3 3 1 2023 SIGMOD PKU Double-Anonymous Sketch: Achieving Top-\ud835\udc3e-fairness for Finding Global Top-\ud835\udc3e Frequent Items double-anonymity technique; randomized admission policy for top-k stage; CMM sketch for count stage 3 4 2 2024 IFIP NPC PKU 2FA Sketch: Two-Factor Armor Sketch for Accurate and Efficient Heavy Hitter Detection in Data Streams improved arbitration strategy for in-bucket competition; cross-bucket conflict avoidance hashing scheme 2 3 1 2024 IEEE ICDE PKU Scalable Overspeed Item Detection in Streams bucket sharing based basic speedsketch algorithm; global-clock for reducing timestamp overhead; counter-flip technique for compression 3 4 2"},{"location":"software/perf/#straggler-analysis","title":"Straggler Analysis","text":"<p>Challenge: stragglers can arise from various complex factors, identifying their root causes and quantifying their impact on performance is difficult.</p> Year Venue Authors Title Tags P E N 2019 TSC BUAA Straggler Root-Cause and Impact Analysis for Massive-scale Virtualized Cloud Datacenters detailing straggler filtration based root cause analysis; DoS-indexf for straggler detection 3 3 1 2020 TJSC QMUL&amp;NUDT Tails in\u00a0the\u00a0cloud: a\u00a0survey and\u00a0taxonomy of\u00a0straggler management within\u00a0large\u2011scale cloud data centres taxonomy of\u00a0straggler causes; straggler management technique 3 1 1 2024 Arxiv HKUST&amp;Alibaba FALCON: Pinpointing and Mitigating Stragglers for Large-Scale Hybrid-Parallel Training Bayesian online change-point detection algorithm; adaptive multi-level mitigation mechanism 4 4 2 2025 Arxiv NYU&amp;ByteDance Understanding Stragglers in Large Model Training Using What-if Analysis what-if analysis; dependency model based simulation; SMon monitoring system 3 4 2"},{"location":"software/perf/#other-bursts","title":"Other Bursts","text":"Year Venue Authors Title Tags P E N 2023 CIKM Edinburgh Tight-Sketch: A High-Performance Sketch for Heavy Item-Oriented Data Stream Mining with Limited Memory Size probabilistic decay strategy; differentiated eviction for cold and hot items 4 4 2 2024 INFOCOM SCU BurstDetector: Real-Time and Accurate Across-Period Burst Detection in High-Speed Networks two-stage across-period burst detection; hierarchical cell for memory optimization 3 4 1"},{"location":"software/perf/#network-tomography","title":"Network Tomography","text":""},{"location":"software/perf/#survey","title":"Survey","text":"Year Venue Authors Title Tags P E N 2004 STAT SCI Berkeley Network Tomography: Recent Developments tomography linear model; multicast delay distribution inference; origin\u2013destination traffic matrix inference 3 1 1"},{"location":"software/perf/#passive-inference","title":"Passive Inference","text":"Year Venue Authors Title Tags P E N 2003 IMC AT&amp;T Laboratories Simple Network Performance Tomography smallest consistent failure set algorithm; seperable performance; false positive/coverage probability estimation of bad links 3 3 3 2014 ICDCS ZJU Domo: Passive Per-Packet Delay Tomography in Wireless Ad-hoc Networks FIFO/order/sum-of-delays constraints for delay reconstruction; semi-definite relaxation based optimization 4 3 2"},{"location":"software/perf/#active-inference","title":"Active Inference","text":"Year Venue Authors Title Tags P E N 2022 ICASSP UMich Unicast-based inference of network link delay distributions using mixed finite mixture models dirac delta based mixed finite mixture model; EM algorithm for parameter evaluation 3 2 2 2003 IEEE TSP Rice University Network Delay Tomography end-to-end packet pair link delay distribution estimation; FFT based expectation-maximization acceleration algorithm 3 3 2 2021 IEEE TNSM QMUL Optimal Estimation of Link Delays Based on End-to-End Active Measurements active network monitoring framework; ILP/heuristic/meta-heuristic algorithm for monitoring flows selection 3 3 2"},{"location":"software/perf/#profiling-techniques","title":"Profiling Techniques","text":""},{"location":"software/perf/#extended-berkeley-packet-filter","title":"Extended Berkeley Packet Filter","text":"<p>Solution: A technique used for dynamically programing the kernel for efficient networking, observability, tracing, and security.</p> Year Venue Authors Title Tags P E N 2024 eBPF THU Understanding Performance of eBPF Maps eBPF map benchmark; impact of cache hotness on eBPF map; volume discount feature of eBPF program 4 4 2 2025 EuroSys UW\u2013Madison Revealing the Unstable Foundations of eBPF-Based Kernel Extensions potential mismatches dataset; dependency surface/set analysis 4 4 2"},{"location":"software/perf/#simulators-and-emulators-for-softwaresystem-analysis","title":"Simulators and emulators (for software/system analysis)","text":"<p>Challenge: how to balance the accuracy, time cost and complexity of a simulator.</p>"},{"location":"software/perf/#general-performance-modeling","title":"General Performance Modeling","text":"<p>Focusing on the performance modeling for general systems. The LLM performance modeling is in the LLM Performance Modeling section.</p> Year Venue Authors Title Tags P E N 2009 CACM Berkeley Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures operational intensity; memory bound; compute bound 2014 IISWC ETH Zurich Extending the Roofline Model: Bottleneck Analysis with Microarchitectural Constraints dag-based performance model; Tomasulo's greedy algorithm; scheduled dag based bottleneck modeling 3 4 3 2021 Intelligent Computing Berkeley Hierarchical Roofline Performance Analysis for Deep Learning Applications Nsight Compute based hierarchical roofline model; FP16\u3001FP32 extension for ERT 2025 arXiv Google Concorde: Fast and Accurate CPU Performance Modeling with Compositional Analytical-ML Fusion per-resource throughput analysis; fine-grained performance attribution 3 2 2"},{"location":"software/perf/#llm-performance-modeling","title":"LLM Performance Modeling","text":"<p>Solution: LLM inference is expensive, performance modeling can help decide on the best configuration for the given system without actually running the LLM.</p>"},{"location":"software/perf/#llm-serving-performance-modeling","title":"LLM Serving Performance Modeling","text":"Year Venue Authors Title Tags P E N 2024 arXiv KAIST LLMServingSim: A HW/SW Co-Simulation Infrastructure for LLM Inference Serving at Scale iteration-level simulation; computation reuse optimization; heterogeneous accelerator mapping 2024 Mlsys GIT Vidur: A Large-Scale Simulation Framework For LLM Inference Operation-level simulation; Using the simulator to search the best configuration for the given system 3 3 3"},{"location":"software/perf/#llm-training-performance-modeling","title":"LLM Training Performance Modeling","text":"Year Venue Authors Title Tags P E N 2025 MLSys Cornell Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training trace-driven performance modeling and estimation toolkit; the first system to provide accurate performance models that effectively capture the execution behaviors of LLMs; modify and generate new execution graphs from existing traces 3 4 2"},{"location":"software/perf/#benchmarking-methodologies-and-suites","title":"Benchmarking methodologies and suites","text":""},{"location":"software/perf/#benchmark","title":"Benchmark","text":"<p>Solution: benchmark targeted at performance analysis and characterization.</p> Year Venue Authors Title Tags P E N 2018 ICPP WUSTL Varbench: an Experimental Framework to Measure and Characterize Performance Variability spatial/temperal variability; Resource Variability (RV) statistic 2021 IEEE Access D-ITET DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks NDP focused workload characterization methodology; memory-bound function identification; locality-based clustering; memory bottlenecks classification"},{"location":"software/pl/","title":"Programming Languages and Software Engineering","text":""},{"location":"software/pl/#language-design-and-semantics","title":"Language design and semantics","text":"<p>Solution: user-friendly, resource-efficient, and secure programming languages</p>"},{"location":"software/pl/#compiler-construction-and-optimization","title":"Compiler construction and optimization","text":"<p>Solution: improving performance, reducing resource usage, and ensuring correctness</p>"},{"location":"software/pl/#deep-learning-compilers","title":"Deep Learning Compilers","text":"<p>Solution: Graph transformations, Kernel fusion, Tensor optimization for compute and memory</p> Year Venue Authors Title Tags P E N 2013 PLDI MIT Halide: A Language and Compiler for Optimizing Parallelism Locality and Recomputation in Image Processing Pipelines image process DSL; Compute and Schedule Separation IR Design Idea; Pipeline optimization 4 4 3 2018 OSDI UW TVM: An Automated End-to-End Optimizing Compiler for Deep Learning operator fusion; graph-level DL compiler; automatic code generation; tensor expression simplification 4 4 4 2025 PPoPP Thu FlashTensor: Optimizing Tensor Programs by Leveraging Fine-grained Tensor Property dataflow centered code recognition and optimization; two-stage heuristic algorithm to optimize tensor computation; kernel fusion 4 4 2 2025 arXiv PKU TileLang: A Composable Tiled Programming Model for AI Systems tile-based programming; tvm-based compiler; compared to trition is more flexible 4 4 3"},{"location":"software/pl/#domain-specific-languages","title":"Domain-Specific Languages","text":"<p>Solution: formal semantics definition, tool generation automation, cross-domain generalization</p>"},{"location":"software/pl/#sparse-tensor-algebra-compilers","title":"Sparse Tensor Algebra Compilers","text":"<p>Solution: multi-format iteration efficiency, format combination optimization, architecture-agnostic code generation</p> Year Venue Authors Title Tags P E N 2018 OOPSLA MIT Format Abstraction for Sparse Tensor Algebra Compilers coordinate hierarchies; level formats abstraction; property-based merge lattice optimizations; level iterator conversion 4 4 3 2020 PLDI MIT Automatic Generation of Efficient Sparse Tensor Format Conversion Routines coordinate remapping notation; attribute query language; tensor assembly abstract interface; three-phase conversion decomposition 4 4 4 2022 OOPSLA MIT Compilation of Dynamic Sparse Tensor Algebra node schema language; assembly abstract interface; map function generation; iterator optimization; dynamic tensor format composition 4 4 3"},{"location":"software/pl/#hardware-description-languages","title":"Hardware Description Languages","text":"<p>Solution: expressive hardware specification, efficient simulation and synthesis, robust verification methodologies</p>"},{"location":"software/pl/#streaming-computation-models","title":"Streaming Computation Models","text":"<p>Solution: high-throughput data processing, real-time analytics, efficient resource utilization for continuous data</p> Year Venue Authors Title Tags P E N 2020 ASPLOS Stanford Fleet: A Framework for Massively Parallel Streaming on FPGAs user write serial code for parallel; multi-stream parallelism; ready-valid signaling 3 4 3"},{"location":"software/pl/#hls-code-generation-and-automation","title":"HLS Code Generation and Automation","text":"<p>Solution: bridging high-level languages to hardware, design space exploration, QoR improvement automation</p>"},{"location":"software/pl/#general-hls-compiler","title":"General HLS Compiler","text":"<p>C/C++/SystemC to RTL, microarchitecture optimization, resource sharing and scheduling</p> Year Venue Authors Title Tags P E N 2022 ASPLOS UCLA HeteroGen: transpiling C to heterogeneous HLS code with automated test generation and program repair automated test generation; dependence-guided search space pruning; early candidate rejection using coding styles 3 4 3 2022 HPCA UIUC ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level Intermediate Representation multi-level IR for HLS; HLS-dedicated analysis/transform library; MLIR-based HLS framework 4 4 4 2022 FPGA Cornell HeteroFlow: An Accelerator Programming Model with Decoupled Data Placement for Software-Defined FPGAs Decoupled data placement; Unified data placement primitive; Multi-level memory hierarchy optimization 4 4 4 2024 DATE UIUC Subgraph Extraction-Based Feedback-Guided Iterative Scheduling for HLS ISDC iterative SDC scheduling; subgraph extraction-based low-level feedback; fanout and window-based subgraph extraction mechanism 4 4 4 2025 FPGA University of Glasgow Dynamic Loop Fusion in High-Level Synthesis Dynamic loop fusion; HLS; Irregular memory access; Address monotonicity; Decoupled Access/Execute (DAE); Program-order schedule; Data Unit (DU) 4 4 4"},{"location":"software/pl/#hls-verification-and-testing","title":"HLS Verification and Testing","text":"<p>Solution: automated bug detection, verification dataset generation, LLM-aided debugging for HLS designs</p> Year Venue Authors Title Tags P E N 2024 LAD UIUC An Iteratively-refined Dataset for High-Level Synthesis Functional Verification through LLM-Aided Bug Injection Chrysalis dataset with bug injection; ICL+RAG+CoT bug injection methodology; iteratively-refined HLS verification dataset 4 4 4"},{"location":"software/pl/#dataflow-hls-acceleration","title":"Dataflow HLS acceleration","text":"<p>Solution: exploiting task-level parallelism, optimizing inter-kernel communication, maximizing pipeline throughput</p> Year Venue Authors Title Tags P E N 2024 ASPLOS UIUC HIDA: A Hierarchical Dataflow Compiler for High-Level Synthesis hierarchical dataflow IR (HIDA-IR); multi-level dataflow optimizer (HIDA-OPT); pattern-driven task fusion 3 5 4 2025 FPGA UCLA Stream-HLS: Towards Automatic Dataflow Acceleration automatic dataflow HLS; global scheduling for streaming; MINLP for HLS optimization 4 4 4"},{"location":"software/pl/#hls-for-specialized-hardware","title":"HLS for specialized hardware","text":"<p>Solution: target-specific code generation, custom memory interface synthesis, co-optimization with physical constraints</p> Year Venue Authors Title Tags P E N 2023 FPGA UoP DONGLE: Direct FPGA-Orchestrated NVMe Storage for HLS HLS direct NVMe access; FPGA-orchestrated storage; Unified HLS storage interface; Single-source HLS for storage; DONGLE architecture 4 4 4 2023 FPGA HKUST FADO: Floorplan-Aware Directive Optimization for High-Level Synthesis Designs on Multi-Die FPGAs Floorplan-aware HLS; Multi-die FPGA optimization; Directive-floorplan co-optimization; Incremental floorplanning for HLS; MMBP for HLS DSE 3 4 4 2025 FPGA Brown University ARIES: An Agile MLIR-Based Compilation Flow for Reconfigurable Devices with AI Engines MLIR-based AIE compilation; Unified AIE+PL IR; Tile-based parallelism; ADF dialect; Automated AIE placement 4 5 4"},{"location":"software/pl/#program-analysis","title":"Program analysis","text":"<p>Solution: statically or dynamically analyzing programs to understand their behavior, detect errors, and optimize performance</p>"},{"location":"software/pl/#domain-specific-program-analysis","title":"Domain-specific program analysis","text":"<p>Solution: leveraging domain knowledge for precise analysis, specialized bug detection, targeted optimization insights</p> Year Venue Authors Title Tags P E N 2024 PPoPP Information Engineering University A Holistic Approach to Automatic Mixed-Precision Code Generation and Tuning for Affine Programs holistic code generation and tuning; polyhedral model for mixed-precision; model-driven autotuning 4 5 4 2024 PPoPP University of Delaware Recurrence Analysis for Automatic Parallelization of Subscripted Subscripts recurrence analysis for parallelization; subscripted subscript analysis; intermittent monotonicity detection 3 4 3"},{"location":"software/pl/#hls-program-analysis","title":"HLS program analysis","text":"<p>Solution: verifying functional correctness of HLS, analyzing performance bottlenecks, ensuring interface compatibility</p> Year Venue Authors Title Tags P E N 2025 FPGA UoE Latency Insensitivity Testing for Dataflow HLS Designs Automated Latency Insensitivity Testing; Parallel Hardware-Accelerated Testing Platform; Test space reduction; Stalling Units (SU) 4 4 4"}]}